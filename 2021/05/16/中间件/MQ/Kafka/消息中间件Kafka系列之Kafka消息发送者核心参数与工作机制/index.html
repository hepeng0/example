<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="消息中间件Kafka系列之Kafka消息发送者核心参数与工作机制"><meta name="keywords" content="Kafka,消息中间件,MQ"><meta name="author" content="何鹏 [smile.hepeng@qq.com]"><meta name="copyright" content="何鹏 [smile.hepeng@qq.com]"><title>消息中间件Kafka系列之Kafka消息发送者核心参数与工作机制 | 沉默者</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.0'
} </script><link rel="alternate" href="/atom.xml" title="沉默者" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Producer-%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E4%B8%80%E8%A7%88"><span class="toc-number">1.</span> <span class="toc-text">Producer 核心流程一览</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#KafkaProducer%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">KafkaProducer构造方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KafkaProducer%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">1.2.</span> <span class="toc-text">KafkaProducer消息发送流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RecordAccumulator"><span class="toc-number">1.2.1.</span> <span class="toc-text">RecordAccumulator</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Sender"><span class="toc-number">1.2.2.</span> <span class="toc-text">Sender</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.2.3.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Producer%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">2.</span> <span class="toc-text">Producer分区器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Producer-%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">Producer 压缩算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Producer-interceptor"><span class="toc-number">4.</span> <span class="toc-text">Producer interceptor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E8%AF%81"><span class="toc-number">5.</span> <span class="toc-text">数据可靠性保证</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="toc-number">5.1.</span> <span class="toc-text">副本数据同步策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ISR"><span class="toc-number">5.2.</span> <span class="toc-text">ISR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ack-%E5%BA%94%E7%AD%94%E6%9C%BA%E5%88%B6"><span class="toc-number">5.3.</span> <span class="toc-text">ack 应答机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="toc-number">5.4.</span> <span class="toc-text">故障处理细节</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%8A%95%E9%80%92%E8%AF%AD%E4%B9%89"><span class="toc-number">6.</span> <span class="toc-text">消息队列投递语义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">其他参数</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://img2.baidu.com/it/u=928705063,3876627980&amp;fm=26&amp;fmt=auto&amp;gp=0.jpg"></div><div class="author-info__name text-center">何鹏 [smile.hepeng@qq.com]</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">51</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">41</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div class="plain" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">沉默者</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div></div><div class="layout" id="content-inner"><article id="post"><div class="plain" id="post-title">消息中间件Kafka系列之Kafka消息发送者核心参数与工作机制</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-05-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"> 中间件</a><span class="post-meta__separator">|</span><span class="post-meta-wordcount"><span>Word count: </span><span class="word-count">6.1k</span><span class="post-meta__separator">|</span><span>Reading time: 22 min</span></span></div><div class="article-container" id="post-content"><p>本文将从Kafka Producer的配置属性为突破口，结合源码深入提炼出Kafka Producer的工作机制，方便大家更好使用Kafka Producer，并且胸有成竹的进行性能调优。</p>
<p>将Kafka Producer相关的参数分成如下几个类型：</p>
<ul>
<li>常规参数</li>
<li>工作原理(性能相关)参数(图解)</li>
</ul>
<p>本文会结合图解方式，重点阐述与Kafka生产者运作机制密切相关的参数。 </p>
<h3 id="Producer-核心流程一览"><a href="#Producer-核心流程一览" class="headerlink" title="Producer 核心流程一览"></a>Producer 核心流程一览</h3><p>producer也就是生产者，是kafka中消息的产生方，产生消息并提交给kafka集群完成消息的持久化。</p>
<h4 id="KafkaProducer构造方法"><a href="#KafkaProducer构造方法" class="headerlink" title="KafkaProducer构造方法"></a>KafkaProducer构造方法</h4><p>KafkaProducer构造方法主要是根据配置文件进行一些实例化操作</p>
<ol>
<li>解析clientId，若没有配置则由是producer-递增的数字</li>
<li>解析并实例化分区器partitioner</li>
<li>解析key、value的序列化方式并实例化</li>
<li>解析并实例化拦截器</li>
<li>解析并实例化RecordAccumulator</li>
<li>解析Broker地址</li>
<li>创建一个Sender线程并启动</li>
</ol>
<details><summary>一个KafkaProducer的小demo</summary>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line">        if (args.length != 2) &#123;</span><br><span class="line">            throw new IllegalArgumentException(&quot;usage: com.ding.KafkaProducerDemo bootstrap-servers topic-name&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        // kafka服务器ip和端口，多个用逗号分割</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, args[0]);</span><br><span class="line">        // 确认信号配置</span><br><span class="line">        // ack=0 代表producer端不需要等待确认信号，可用性最低</span><br><span class="line">        // ack=1 等待至少一个leader成功把消息写到log中，不保证follower写入成功，如果leader宕机同时follower没有把数据写入成功</span><br><span class="line">        // 消息丢失</span><br><span class="line">        // ack=all leader需要等待所有follower成功备份，可用性最高</span><br><span class="line">        props.put(&quot;ack&quot;, &quot;all&quot;);</span><br><span class="line">        // 重试次数</span><br><span class="line">        props.put(&quot;retries&quot;, 0);</span><br><span class="line">        // 批处理消息的大小，批处理可以增加吞吐量</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">        // 延迟发送消息的时间</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">        // 用来换出数据的内存大小</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">        // key 序列化方式</span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        // value 序列化方式</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        // 创建KafkaProducer对象，创建时会启动Sender线程</span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        for (int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">            // 往RecordAccumulator中写消息</span><br><span class="line">            Future&lt;RecordMetadata&gt; result = producer.send(new ProducerRecord&lt;&gt;(args[1], Integer.toString(i), Integer.toString(i)));</span><br><span class="line">            RecordMetadata rm = result.get();</span><br><span class="line">            System.out.println(&quot;topic: &quot; + rm.topic() + &quot;, partition: &quot; +  rm.partition() + &quot;, offset: &quot; + rm.offset());</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</details>

<p>相关参数:</p>
<ul>
<li><strong>bootstrap.servers</strong>: 配置Kafka broker的服务器地址列表，多个用英文逗号分开，可以不必写全，Kafka内部有自动感知Kafka broker的机制。</li>
<li><strong>key.serializer</strong>: 消息key的序列化策略，为org.apache.kafka.common.serialization接口的实现类。</li>
<li><strong>value.serializer</strong>: 消息体的序列化策略</li>
<li><strong>client.id</strong>: 客户端ID，如果不设置默认为producer-递增，<strong>强烈建议设置该值，尽量包含ip,port,pid</strong>。</li>
<li><strong>client.dns.lookup</strong>: 客户端寻找bootstrap地址的方式，支持如下两种方式：<ul>
<li><strong>resolve_canonical_bootstrap_servers_only</strong>: 这种方式，会依据bootstrap.servers提供的主机名(hostname)，根据主机上的名称服务返回其IP地址的数组(InetAddress.getAllByName)，然后依次获取inetAddress.getCanonicalHostName()，再建立tcp连接。<br><strong>一个主机可配置多个网卡，如果启用该功能，应该可以有效利用多网卡的优势，降低Broker的网络端负载压力。</strong></li>
<li><strong>use_all_dns_ips</strong>: 这种方式会直接使用bootstrap.servers中提供的hostname、port创建tcp连接，默认选项。</li>
</ul>
</li>
</ul>
<h4 id="KafkaProducer消息发送流程"><a href="#KafkaProducer消息发送流程" class="headerlink" title="KafkaProducer消息发送流程"></a>KafkaProducer消息发送流程</h4><p>Kafka将一条待发送的消息抽象为ProducerRecord对象，其数据结构是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic; <span class="comment">//目标topic</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition; <span class="comment">//目标partition</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;<span class="comment">//消息头信息</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;   <span class="comment">//消息key</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value; <span class="comment">//消息体</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp; <span class="comment">//消息时间戳</span></span><br><span class="line">    <span class="comment">//省略构造方法与成员方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目前消息结构包括6个核心属性，分别是topic，partition，headers，key，value与timestamp，各属性含义如上也比较好理解，其中headers属性是Kafka 0.11.x 版本引入的，可以用它存储一些应用或业务相关的信息。</p>
<p>Kafka消息发送过程中主要涉及ProducerRecord对象的构建、分区选择、元数据的填充、ProducerRecord对象的序列化、进入消息缓冲池、完成消息的发送、接受broker的响应。</p>
<p>消息的发送入口是KafkaProducer.send方法，具体流程如下: </p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_3.png"></p>
<ol>
<li>确定topic信息</li>
<li>确定value信息</li>
<li>然后进行消息的序列化处理</li>
<li>由分区选择器确定对应的分区信息</li>
<li>将消息写入消息缓冲区</li>
<li>完成消息请求的发送</li>
<li>完成消息响应的处理</li>
</ol>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img.png"></p>
<p>总的来说，Kafka生产端发送数据过程涉及到序列化器Serializer、分区器Partitioner，消息缓存池Accumulator，还可能会涉及到拦截器Interceptor（这部分暂不做介绍）。</p>
<p>Kafka 的 Producer 发送消息采用的是异步发送的方式。</p>
<p>在消息发送的过程中，涉及到了 两个线程——<strong>main线程</strong>和 <strong>Sender线程</strong>，以及<strong>一个线程共享变量——RecordAccumulator。 main 线程将消息发送给 RecordAccumulator</strong>，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_4.png"></p>
<p>在消息发送端Kafka引入了批的概念，发送到服务端的消息通常不是一条一条发送，而是一批一批发送，一个批次对应源码层级为ProducerBatch对象。<br>相关参数：</p>
<ul>
<li><strong>batch.size</strong><br>该值用于设置每一个批次的内存大小,默认为16K,只有数据积累到 batch.size 之后，sender 才会发送数据。</li>
<li><strong>linger.ms</strong>:<br>Kafka希望一个批次一个批次去发送到Broker，应用程序往KafkaProducer中发送一条消息，首先会进入到内部缓冲区，具体是会进入到某一个批次中(ProducerBatch), 等待该批次堆满后一次发送到Broker，这样能提高消息的吞吐量，但其消息发送的延迟也会相应提高。<br>为了解决该问题，linger.ms参数应运而生。<br>它的作用是控制在缓存区中未积满时来控制消息发送线程的行为。 如果linger.ms 设置为 0表示立即发送，如果设置为大于0，则消息发送线程会等待这个值后才会向broker发送。有点类似于 TCP 领域的 Nagle 算法。.如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。</li>
</ul>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_1.png"></p>
<p>Kafka的每一个消息发送者，也就是KafkaProducer对象内部会有一块缓存区，缓冲区内存的组织会按照topic+parition构建双端队列。<br>相关参数：</p>
<ul>
<li><strong>buffer.memory</strong>:<br>指定缓存区大小，默认为32M</li>
<li><strong>delivery.timeout.ms</strong>:<br>默认为120s，该参数控制在双端队列中的过期时间，从进入双端队列开始计时，超过该值未被sender发送后会返回超时异常(TimeoutException)。</li>
</ul>
<p>队列中的每一个元素为一个ProducerBatch对象，表示一个消息发送批次，但发送线程将消息发送到Broker端时，一次可以包含多个批次。</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_2.png"></p>
<p>相关参数：</p>
<ul>
<li><p><strong>max.block.ms</strong>:<br>默认为60s，当消息发送者申请空闲内存时，如果在指定时间（包含发送端用于查找元信息的时间）内未申请到内存，消息发送端会直接报TimeoutException。</p>
</li>
<li><p><strong>max.request.size</strong>:<br>Send线程一次发送的最大字节数量，也就是Send线程向服务端一次消息发送请求的最大传输数据，默认为1M。</p>
</li>
<li><p><strong>request.timeout.ms</strong>:<br>请求的超时时间，主要是Kafka消息发送线程(Sender)与Broker端的网络通讯的请求超时时间。</p>
</li>
<li><p><strong>retries</strong>:<br>Kafka Sender线程从缓存区尝试发送到Broker端的重试次数，默认为Integer.MAX_VALUE。<br>为了避免无限重试，只针对可恢复的异常，例如Leader选举中这种异常就是可恢复的，重试最终是能解决问题的。</p>
</li>
<li><p><strong>max.in.flight.requests.per.connection</strong>:<br>设置每一个客户端与服务端连接，在应用层一个通道的积压消息数量，默认为5，有点类似Netty用高低水位线控制发送缓冲区中积压的多少，避免内存溢出。</p>
</li>
</ul>
<h5 id="RecordAccumulator"><a href="#RecordAccumulator" class="headerlink" title="RecordAccumulator"></a>RecordAccumulator</h5><p>RecordAccumulator是消息队列用于缓存消息，根据TopicPartition对消息分组</p>
<details><summary>RecordAccumulator源码解读</summary>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Add a record to the accumulator, return the append result</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The append result will contain the future metadata, and flag for whether the appended batch is full or a new batch is created</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> *</span><br><span class="line"> * @param tp The topic/partition to which this record is being sent</span><br><span class="line"> * @param timestamp The timestamp of the record</span><br><span class="line"> * @param key The key for the record</span><br><span class="line"> * @param value The value for the record</span><br><span class="line"> * @param headers the Headers for the record</span><br><span class="line"> * @param callback The user-supplied callback to execute when the request is complete</span><br><span class="line"> * @param maxTimeToBlock The maximum time in milliseconds to block for buffer memory to be available</span><br><span class="line"> */</span><br><span class="line">public RecordAppendResult append(TopicPartition tp,</span><br><span class="line">                                 long timestamp,</span><br><span class="line">                                 byte[] key,</span><br><span class="line">                                 byte[] value,</span><br><span class="line">                                 Header[] headers,</span><br><span class="line">                                 Callback callback,</span><br><span class="line">                                 long maxTimeToBlock) throws InterruptedException &#123;</span><br><span class="line">    // We keep track of the number of appending thread to make sure we do not miss batches in</span><br><span class="line">    // abortIncompleteBatches().</span><br><span class="line">    // ---记录进行applend的线程数---</span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    ByteBuffer buffer = null;</span><br><span class="line">    if (headers == null) headers = Record.EMPTY_HEADERS;</span><br><span class="line">    try &#123;</span><br><span class="line">        // check if we have an in-progress batch</span><br><span class="line">        // ---根据TopicPartition获取或新建Deque双端队列---</span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        // ---尝试将消息加入到缓冲区中---</span><br><span class="line">        // ---加锁保证同一个TopicPartition写入有序---</span><br><span class="line">        synchronized (dq) &#123;</span><br><span class="line">            if (closed)</span><br><span class="line">                throw new KafkaException(&quot;Producer closed while send in progress&quot;);</span><br><span class="line">            // 尝试写入</span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            if (appendResult != null)</span><br><span class="line">                return appendResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // we don&#x27;t have an in-progress record batch try to allocate a new batch</span><br><span class="line">        byte maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(&quot;Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;&quot;, size, tp.topic(), tp.partition());</span><br><span class="line">        // 尝试applend失败（返回null），会走到这里。如果tryApplend成功直接返回了</span><br><span class="line">        // 从BufferPool中申请内存空间，用于创建新的ProducerBatch</span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line">        synchronized (dq) &#123;</span><br><span class="line">            // Need to check if producer is closed again after grabbing the dequeue lock.</span><br><span class="line">            if (closed)</span><br><span class="line">                throw new KafkaException(&quot;Producer closed while send in progress&quot;);</span><br><span class="line">            </span><br><span class="line">            // 注意这里，前面已经尝试添加失败了，且已经分配了内存，为何还要尝试添加？</span><br><span class="line">            // 因为可能已经有其他线程创建了ProducerBatch或者之前的ProducerBatch已经被Sender线程释放了一些空间，所以在尝试添加一次。这里如果添加成功，后面会在finally中释放申请的空间</span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            if (appendResult != null) &#123;</span><br><span class="line">                // Somebody else found us a batch, return the one we waited for! Hopefully this doesn&#x27;t happen often...</span><br><span class="line">                return appendResult;</span><br><span class="line">            &#125;</span><br><span class="line">            // 尝试添加失败了，新建ProducerBatch</span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            // 将buffer置为null,避免在finally汇总释放空间</span><br><span class="line">            // Don&#x27;t deallocate this buffer in the finally block as it&#x27;s being used in the record batch</span><br><span class="line">            buffer = null;</span><br><span class="line">            return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        // 最后如果再次尝试添加成功，会释放之前申请的内存（为了新建ProducerBatch）</span><br><span class="line">        if (buffer != null)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers,</span><br><span class="line">                                     Callback callback, Deque&lt;ProducerBatch&gt; deque) &#123;</span><br><span class="line">    // 从双端队列的尾部取出ProducerBatch</span><br><span class="line">    ProducerBatch last = deque.peekLast();</span><br><span class="line">    if (last != null) &#123;</span><br><span class="line">        // 取到了，尝试添加消息</span><br><span class="line">        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, time.milliseconds());</span><br><span class="line">        // 空间不够，返回null</span><br><span class="line">        if (future == null)</span><br><span class="line">            last.closeForRecordAppends();</span><br><span class="line">        else</span><br><span class="line">            return new RecordAppendResult(future, deque.size() &gt; 1 || last.isFull(), false);</span><br><span class="line">    &#125;</span><br><span class="line">    // 取不到返回null</span><br><span class="line">    return null;</span><br><span class="line">&#125;</span><br><span class="line">public FutureRecordMetadata tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, long now) &#123;</span><br><span class="line">    // 空间不够，返回null</span><br><span class="line">    if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) &#123;</span><br><span class="line">        return null;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // 真正添加消息</span><br><span class="line">        Long checksum = this.recordsBuilder.append(timestamp, key, value, headers);</span><br><span class="line">        ...</span><br><span class="line">        FutureRecordMetadata future = ...</span><br><span class="line">        // future和回调callback进行关联    </span><br><span class="line">        thunks.add(new Thunk(callback, future));</span><br><span class="line">        ...</span><br><span class="line">        return future;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 将消息写入缓冲区</span><br><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line">if (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">    // 缓冲区满了或者新创建的ProducerBatch，唤起Sender线程</span><br><span class="line">    this.sender.wakeup();</span><br><span class="line">&#125;</span><br><span class="line">return result.future;</span><br></pre></td></tr></table></figure>

</details>

<h5 id="Sender"><a href="#Sender" class="headerlink" title="Sender"></a>Sender</h5><p>KafkaProducer的构造方法在实例化时启动一个KafkaThread线程来执行Sender</p>
<p>Sender主要流程如下： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Sender.run</span><br><span class="line">Sender.runOnce</span><br><span class="line">Sender.sendProducerData</span><br><span class="line">// 获取集群信息</span><br><span class="line">Metadata.fetch</span><br><span class="line">// 获取可以发送消息的分区且已经获取到了leader分区的节点</span><br><span class="line">RecordAccumulator.ready</span><br><span class="line">// 根据准备好的节点信息从缓冲区中获取topicPartion对应的Deque队列中取出ProducerBatch信息</span><br><span class="line">RecordAccumulator.drain</span><br><span class="line">// 将消息转移到每个节点的生产请求队列中</span><br><span class="line">Sender.sendProduceRequests</span><br><span class="line">// 为消息创建生产请求队列</span><br><span class="line">Sender.sendProducerRequest</span><br><span class="line">KafkaClient.newClientRequest</span><br><span class="line">// 下面是发送消息</span><br><span class="line">KafkaClient.sent</span><br><span class="line">NetWorkClient.doSent</span><br><span class="line">Selector.send</span><br><span class="line">// 其实上面并不是真正执行I/O，只是写入到KafkaChannel中</span><br><span class="line">// poll 真正执行I/O</span><br><span class="line">KafkaClient.poll</span><br></pre></td></tr></table></figure>

<details><summary></summary>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// KafkaProducer构造方法启动Sender</span><br><span class="line">String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;</span><br><span class="line">this.ioThread = new KafkaThread(ioThreadName, this.sender, true);</span><br><span class="line">this.ioThread.start();</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Sender-&gt;run()-&gt;runOnce()</span><br><span class="line">long currentTimeMs = time.milliseconds();</span><br><span class="line">// 发送生产的消息</span><br><span class="line">long pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">// 真正执行I/O操作</span><br><span class="line">client.poll(pollTimeout, currentTimeMs);</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 获取集群信息</span><br><span class="line">Cluster cluster = metadata.fetch();</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 获取准备好可以发送消息的分区且已经获取到leader分区的节点</span><br><span class="line">RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);</span><br><span class="line">// ReadyCheckResult 包含可以发送消息且获取到leader分区的节点集合、未获取到leader分区节点的topic集合</span><br><span class="line">public final Set&lt;Node&gt; 的节点;</span><br><span class="line">public final long nextReadyCheckDelayMs;</span><br><span class="line">public final Set&lt;String&gt; unknownLeaderTopics;</span><br></pre></td></tr></table></figure>

<p>ready方法主要是遍历在上面介绍RecordAccumulator添加消息的容器，Map&lt;TopicPartition, Deque&gt;，从集群信息中根据TopicPartition获取leader分区所在节点，找不到对应leader节点但有要发送的消息的topic添加到unknownLeaderTopics中。同时把那些根据TopicPartition可以获取leader分区且消息满足发送的条件的节点添加到的节点中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 遍历batches</span><br><span class="line">for (Map.Entry&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; entry : this.batches.entrySet()) &#123;</span><br><span class="line">    TopicPartition part = entry.getKey();</span><br><span class="line">    Deque&lt;ProducerBatch&gt; deque = entry.getValue();</span><br><span class="line">    // 根据TopicPartition从集群信息获取leader分区所在节点</span><br><span class="line">    Node leader = cluster.leaderFor(part);</span><br><span class="line">    synchronized (deque) &#123;</span><br><span class="line">        if (leader == null &amp;&amp; !deque.isEmpty()) &#123;</span><br><span class="line">            // 添加未找到对应leader分区所在节点但有要发送的消息的topic</span><br><span class="line">            unknownLeaderTopics.add(part.topic());</span><br><span class="line">        &#125; else if (!readyNodes.contains(leader) &amp;&amp; !isMuted(part, nowMs)) &#123;</span><br><span class="line">				....</span><br><span class="line">                if (sendable &amp;&amp; !backingOff) &#123;</span><br><span class="line">                    // 添加准备好的节点</span><br><span class="line">                    readyNodes.add(leader);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后对返回的unknownLeaderTopics进行遍历，将topic加入到metadata信息中，调用metadata.requestUpdate方法请求更新metadata信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for (String topic : result.unknownLeaderTopics)</span><br><span class="line">    this.metadata.add(topic);</span><br><span class="line">    result.unknownLeaderTopics);</span><br><span class="line">	this.metadata.requestUpdate();</span><br></pre></td></tr></table></figure>
<p>对已经准备好的节点进行最后的检查，移除那些节点连接没有就绪的节点，主要根据KafkaClient.ready方法进行判断</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">long notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">while (iter.hasNext()) &#123;</span><br><span class="line">    Node node = iter.next();</span><br><span class="line">    // 调用KafkaClient.ready方法验证节点连接是否就绪</span><br><span class="line">    if (!this.client.ready(node, now)) &#123;</span><br><span class="line">        // 移除没有就绪的节点</span><br><span class="line">        iter.remove();</span><br><span class="line">        notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面开始创建生产消息的请求</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 从RecordAccumulator中取出TopicPartition对应的Deque双端队列，然后从双端队列头部取出ProducerBatch，作为要发送的信息</span><br><span class="line">Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);</span><br></pre></td></tr></table></figure>
<p>把消息封装成ClientRequest</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,requestTimeoutMs, callback);</span><br></pre></td></tr></table></figure>
<p>调用KafkaClient发送消息（并非真正执行I/O），涉及到KafkaChannel。Kafka的通信采用的是NIO方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// NetworkClient.doSent方法</span><br><span class="line">String destination = clientRequest.destination();</span><br><span class="line">RequestHeader header = clientRequest.makeHeader(request.version());</span><br><span class="line">...</span><br><span class="line">Send send = request.toSend(destination, header);</span><br><span class="line">InFlightRequest inFlightRequest = new InFlightRequest(clientRequest,header,isInternalRequest,request,send,now);</span><br><span class="line">this.inFlightRequests.add(inFlightRequest);</span><br><span class="line">selector.send(send);</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// Selector.send方法    </span><br><span class="line">String connectionId = send.destination();</span><br><span class="line">KafkaChannel channel = openOrClosingChannelOrFail(connectionId);</span><br><span class="line">if (closingChannels.containsKey(connectionId)) &#123;</span><br><span class="line">    this.failedSends.add(connectionId);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        channel.setSend(send);</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>到这里，发送消息的工作准备的差不多了，调用KafkaClient.poll方法，真正执行I/O操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.poll(pollTimeout, currentTimeMs);</span><br></pre></td></tr></table></figure>
</details>

<p>用一张图总结Sender线程的流程</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_6.png"></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>Kafka生产消息的主要流程，涉及到主线程往RecordAccumulator中写入消息，同时后台的Sender线程从RecordAccumulator中获取消息，使用NIO的方式把消息发送给Kafka，用一张图总结</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_5.png"></p>
<h3 id="Producer分区器"><a href="#Producer分区器" class="headerlink" title="Producer分区器"></a>Producer分区器</h3><p>分区器partitioner，可以实现自己的partitioner，比如根据key分区，可以保证相同key分到同一个分区，对保证顺序很有用。</p>
<p>相关参数：</p>
<ul>
<li><strong>partitioner.class</strong>:<br>消息发送队列负载算法，其默 DefaultPartitioner，路由算法如下：<ul>
<li>如果指定了 key，则使用 key 的 hashcode 与分区数取模。</li>
<li>如果未指定 key，则轮询所有的分区(用随机数对可用分区取模, counter值初始值是随机的，但后面都是递增的，所以可以算到roundrobin)。</li>
</ul>
</li>
</ul>
<h3 id="Producer-压缩算法"><a href="#Producer-压缩算法" class="headerlink" title="Producer 压缩算法"></a>Producer 压缩算法</h3><p>Kafka支持的压缩算法还是很可观的：GZIP、Snappy、LZ4，默认情况下不进行消息压缩，毕竟会消耗很大一部分cpu时间，导致send方法处理时间变慢。启动LZ4 进行消息压缩的producer的吞吐量是最高的。</p>
<p><strong>发送方与Broker 服务器采用相同的压缩类型，可有效避免在Broker服务端进行消息的压缩与解压缩，大大降低Broker的CPU使用压力</strong></p>
<p>相关参数：</p>
<ul>
<li><strong>compression.type</strong>:<br>消息的压缩算法，目前可选值：none、gzip、snappy、lz4、zstd，<strong>默认不压缩，建议与Kafka服务器配置的一样</strong>。</li>
</ul>
<p>当然Kafka服务端可以配置的压缩类型为 producer，即采用与发送方配置的压缩类型。</p>
<h3 id="Producer-interceptor"><a href="#Producer-interceptor" class="headerlink" title="Producer interceptor"></a>Producer interceptor</h3><p>拦截器是新版本才出现的一个特性，并且是非必须的。</p>
<p>interceptor 核心的函数有: </p>
<ul>
<li>onSend（在消息序列化计算分区之前就被调用）</li>
<li>onAcknowleagement（被应答前或者说在发送失败时，这个方法是运行在producer的I/O线程中的，所以说如果存在很多重逻辑的话会导致严重影响处理消息的速率）</li>
<li>close。通常是通过为clients定制一部分通用且简单的逻辑时才会使用的。</li>
</ul>
<p>相关参数: </p>
<ul>
<li><strong>interceptor.classes</strong>:<br>拦截器列表，kafka运行在消息真正发送到broker之前对消息进行拦截加工。</li>
</ul>
<h3 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h3><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后都需要向producer发送ack(acknowledgement 确认收到)，如果producer收到ack,就会进行下一轮的发送，否则重新发送数据。</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_7.png"></p>
<h4 id="副本数据同步策略"><a href="#副本数据同步策略" class="headerlink" title="副本数据同步策略"></a>副本数据同步策略</h4><table>
<thead>
<tr>
<th align="left">方案</th>
<th align="left">优点</th>
<th align="left">缺点</th>
</tr>
</thead>
<tbody><tr>
<td align="left">半数以上完成同步，就发送ack</td>
<td align="left">延迟低</td>
<td align="left">选举新的leader时，容忍n台节点故障，需要2n+1个副本</td>
</tr>
<tr>
<td align="left">全部完成同步，才发送ack</td>
<td align="left">选举新的leader时，容忍n台节点故障，需要n+1个副本</td>
<td align="left">延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<p>同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p>
<p>虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。</p>
<h4 id="ISR"><a href="#ISR" class="headerlink" title="ISR"></a>ISR</h4><p>采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？</p>
<p>Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower 长时间未向 leader 同步数据 ， 则该 follower 将被踢出ISR ， 该时间阈值由replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。</p>
<h4 id="ack-应答机制"><a href="#ack-应答机制" class="headerlink" title="ack 应答机制"></a>ack 应答机制</h4><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。</p>
<p>所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<ul>
<li>0: 表示生产者不关心该条消息在 broker 端的处理结果，只要调用 KafkaProducer 的 send 方法返回后即认为成功，显然这种方式是最不安全的，因为 Broker 端可能压根都没有收到该条消息或存储失败。</li>
<li>1: 等待至少一个leader成功把消息写到log中，不保证follower写入成功，如果leader宕机同时follower没有把数据写入成功，数据丢失。</li>
<li>all 或 -1: 表示消息不仅需要 Leader 节点已存储该消息，并且要求其副本（准确的来说是 ISR 中的节点）全部存储才认为已提交，才向客户端返回提交成功。这是最严格的持久化保障，当然性能也最低。<ul>
<li>但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。</li>
</ul>
</li>
</ul>
<p>相关参数:</p>
<ul>
<li><strong>acks</strong>:<br>ack应答级别</li>
</ul>
<h4 id="故障处理细节"><a href="#故障处理细节" class="headerlink" title="故障处理细节"></a>故障处理细节</h4><p>Log文件中的HW和LEO</p>
<p><img src="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%80%85%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/img_8.png"></p>
<p>LEO：指的是每个副本最大的 offset；<br>HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。</p>
<ul>
<li>follower 故障: follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘 记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。 等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重 新加入 ISR 了</li>
<li>leader 故障: leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。</li>
</ul>
<p><strong>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</strong></p>
<h3 id="消息队列投递语义"><a href="#消息队列投递语义" class="headerlink" title="消息队列投递语义"></a>消息队列投递语义</h3><ul>
<li>At Least Once 可以保证数据不丢失，但是不能保证数据不重复。</li>
<li>At Most Once 可以保证数据不重复，但是不能保证数据不丢失。</li>
<li>对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。</li>
</ul>
<p>Kafka投递语义实现方案：</p>
<ul>
<li><p>将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。</p>
</li>
<li><p>相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 At Most Once 语义。</p>
</li>
</ul>
<p>在 0.11 版本以前的 Kafka，对Exactly Once 语义是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。</p>
<p>对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p>
<p>0.11 版本的 Kafka，引入了一项重大特性：<strong>幂等性</strong>。</p>
<pre><code>所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据，Server 端都只会持久化一条。
</code></pre>
<p>幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即：</p>
<p><strong>At Least Once + 幂等性 = Exactly Once</strong></p>
<p>相关参数: </p>
<ul>
<li><strong>enable.idempotence</strong>: 是否开启发送端的幂等，默认为false。</li>
<li><strong>acks</strong>: all</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。 </span><br><span class="line">开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。</span><br><span class="line">而 Broker 端会对做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。</span><br></pre></td></tr></table></figure>

<p>但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以<strong>幂等性无法保证分区跨会话的 Exactly Once</strong></p>
<h3 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h3><ul>
<li><strong>send.buffer.bytes</strong>: 网络通道(TCP)的发送缓存区大小，默认为128K。</li>
<li><strong>receive.buffer.bytes</strong>: 网络通道(TCP)的接收缓存区大小，默认为32K。</li>
<li><strong>reconnect.backoff.ms</strong>: 重新建立链接的等待时长，默认为50ms，属于底层网络参数，基本无需关注。</li>
<li><strong>reconnect.backoff.max.ms</strong>: 重新建立链接的最大等待时长，默认为1s，连续两次对同一个连接建立重连，等待时间会在reconnect.backoff.ms的初始值上成指数级递增，但超过max后，将不再指数级递增。</li>
<li><strong>transaction.timeout.ms</strong>: 事务协调器等待客户端的事务状态反馈的最大超时时间，默认为60s。</li>
<li><strong>transactional.id</strong>: 事务id,用于在一个事务中唯一标识一个客户端</li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a><a class="post-meta__tags" href="/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/">消息中间件</a><a class="post-meta__tags" href="/tags/MQ/">MQ</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/05/20/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96%E6%9C%BA%E5%88%B6%E7%AE%80%E8%AF%BB/"><i class="fa fa-chevron-left">  </i><span>消息中间件Kafka系列之Kafka消息拉取机制简读</span></a></div><div class="next-post pull-right"><a href="/2021/05/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/MQ/Kafka/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86/"><span>消息中间件Kafka系列之Kafka复制原理</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '83850031ee87c27765eb',
  clientSecret: '6c5e13a8b6670d5bfa85ba548ba3570cae29e671',
  repo: 'blob-comment',
  owner: 'hepeng0',
  admin: 'hepeng0',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2022 By 何鹏 [smile.hepeng@qq.com]</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>