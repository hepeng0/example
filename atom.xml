<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>沉默者</title>
  
  <subtitle>学会了喋喋不休-习惯了沉默不语</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-07-12T06:45:09.254Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>hepeng [smile.hepeng@qq.com]</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>分布式系统04之分布式锁</title>
    <link href="http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F04%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"/>
    <id>http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F04%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</id>
    <published>2021-07-09T10:35:00.000Z</published>
    <updated>2021-07-12T06:45:09.254Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是分布式锁"><a href="#什么是分布式锁" class="headerlink" title="什么是分布式锁"></a>什么是分布式锁</h2><ul><li>分布式模型下，数据只有一份，需要锁技术控制某一时刻修改数据的进程数。 </li><li>不仅需要保证进程可见，还需要考虑进程与锁的网络问题 </li><li>可以将标记存在内存，但是内存不是进程分配而是公共内存（redis、zk）,保证标记互斥。</li></ul><h2 id="Java分布式锁需求"><a href="#Java分布式锁需求" class="headerlink" title="Java分布式锁需求"></a>Java分布式锁需求</h2><ul><li>同一个方法在同一时间只能被一台机器上一个线程执行。</li><li>可重入（避免死锁）</li><li>阻塞锁（业务需求） </li><li>公平锁（业务需求） </li><li>高可用、高性能获取/释放锁</li></ul><h2 id="Java分布式锁解决方案"><a href="#Java分布式锁解决方案" class="headerlink" title="Java分布式锁解决方案"></a>Java分布式锁解决方案</h2><h3 id="基于数据库"><a href="#基于数据库" class="headerlink" title="基于数据库"></a>基于数据库</h3><p>基于表主键唯一做分布式锁</p><h3 id="基于redis"><a href="#基于redis" class="headerlink" title="基于redis"></a>基于redis</h3><h4 id="基于-redis-的-SETNX-、EXPIRE-方法做分布式锁"><a href="#基于-redis-的-SETNX-、EXPIRE-方法做分布式锁" class="headerlink" title="基于 redis 的 SETNX()、EXPIRE() 方法做分布式锁"></a>基于 redis 的 SETNX()、EXPIRE() 方法做分布式锁</h4><p>使用步骤：</p><ul><li>setnx(lockkey, 1) 返回1，占位成功</li><li>expire()对lockkey设置超时时间，避免死锁</li><li>执行完业务后，delete命令删除key</li></ul><p>在 expire() 命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题。</p><h4 id="基于-redis-的-setnx-、get-和-getset-方法来实现分布式锁。"><a href="#基于-redis-的-setnx-、get-和-getset-方法来实现分布式锁。" class="headerlink" title="基于 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。"></a>基于 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。</h4><p>使用步骤</p><ul><li>setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。</li><li>get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。</li><li>计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。</li><li>判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。</li><li>在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。</li></ul><h4 id="分布式锁Redlock"><a href="#分布式锁Redlock" class="headerlink" title="分布式锁Redlock"></a>分布式锁Redlock</h4><p>解决问题：<br>解决redis分布式锁的单点故障问题</p><p>使用步骤：</p><ul><li>获取当前时间（毫秒数）。</li><li>按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。</li><li>计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（&gt;= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。</li><li>如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。</li><li>如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的Redis Lua脚本）。</li></ul><h4 id="基于-REDISSON-做分布式锁"><a href="#基于-REDISSON-做分布式锁" class="headerlink" title="基于 REDISSON 做分布式锁"></a>基于 REDISSON 做分布式锁</h4><p>redis 官方的分布式锁组件，解决超时时间设置不合理问题。每获得一个锁时，只设置一个很短的超时时间，同时起一个线程在每次快要到超时时间时去刷新锁的超时时间。在释放锁的同时结束这个线程。</p><h3 id="zookeeper实现分布式锁"><a href="#zookeeper实现分布式锁" class="headerlink" title="zookeeper实现分布式锁"></a>zookeeper实现分布式锁</h3><p>其实基于ZooKeeper，就是使用它的临时有序节点来实现的分布式锁。</p><p><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F04%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/img_1.png" alt="img.png"><br>当某客户端要进行逻辑的加锁时，就在zookeeper上的某个指定节点的目录下，去生成一个唯一的临时有序节点， 然后判断自己是否是这些有序节点中序号最小的一个。</p><ul><li>如果是，则算是获取了锁。</li><li>如果不是，则说明没有获取到锁，那么就需要在序列中找到比自己小的那个节点，并对其调用exist()方法，对其注册事件监听，当监听到这个节点被删除了，那就再去判断一次自己当初创建的节点是否变成了序列中最小的。<ul><li>如果是，则获取锁，如果不是，则重复上述步骤。</li></ul></li></ul><p>当释放锁的时候，只需将这个临时节点删除即可。</p><h2 id="redis分布式锁和zookeeper分布式锁的区别"><a href="#redis分布式锁和zookeeper分布式锁的区别" class="headerlink" title="redis分布式锁和zookeeper分布式锁的区别"></a>redis分布式锁和zookeeper分布式锁的区别</h2><h3 id="优缺点对比"><a href="#优缺点对比" class="headerlink" title="优缺点对比"></a>优缺点对比</h3><p>对于redis的分布式锁而言：</p><ul><li><p>它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。</p></li><li><p>redis的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮</p><ul><li>即便使用redlock算法来实现，在某些复杂场景下，也无法保证其实现100%没有问题，关于redlock的讨论可以看How to do distributed locking</li></ul></li></ul><p>但是另一方面使用redis实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的“极端复杂场景”</p><p>所以使用redis作为分布式锁也不失为一种好的方案，最重要的一点是redis的性能很高，可以支撑高并发的获取、释放锁操作。</p><p>对于zk分布式锁而言:</p><ul><li><p>zookeeper天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。</p></li><li><p>如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。</p></li></ul><p>但是zk也有其缺点：如果有较多的客户端频繁的申请加锁、释放锁，对于zk集群的压力会比较大。</p><h3 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h3><p>就个人而言的话，我<strong>比较推崇zk实现的锁</strong>：</p><p>因为redis是有可能存在隐患的，可能会导致数据不对的情况。但是，怎么选用要看具体在公司的场景了。</p><p>如果公司里面有zk集群条件，优先选用zk实现，但是如果说公司里面只有redis集群，没有条件搭建zk集群。</p><p>那么其实用redis来实现也可以，另外还可能是系统设计者考虑到了系统已经有redis，但是又不希望再次引入一些外部依赖的情况下，可以选用redis。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是分布式锁&quot;&gt;&lt;a href=&quot;#什么是分布式锁&quot; class=&quot;headerlink&quot; title=&quot;什么是分布式锁&quot;&gt;&lt;/a&gt;什么是分布式锁&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;分布式模型下，数据只有一份，需要锁技术控制某一时刻修改数据的进程数。 &lt;/li&gt;
&lt;li</summary>
      
    
    
    
    
    <category term="分布式系统" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统03之分布式事务</title>
    <link href="http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
    <id>http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</id>
    <published>2021-07-09T10:34:43.000Z</published>
    <updated>2021-07-12T08:55:44.400Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是分布式事务"><a href="#什么是分布式事务" class="headerlink" title="什么是分布式事务"></a>什么是分布式事务</h2><p>分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。</p><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。</p><h2 id="分布式系统一致性基础算法"><a href="#分布式系统一致性基础算法" class="headerlink" title="分布式系统一致性基础算法"></a>分布式系统一致性基础算法</h2><h3 id="Paxos算法"><a href="#Paxos算法" class="headerlink" title="Paxos算法"></a>Paxos算法</h3><p>Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致 。</p><p>在 Paxos 中主要有三个角色，分别为 Proposer提案者、Acceptor表决者、Learner学习者。Paxos 算法和 2PC 一样，也有两个阶段，分别为 Prepare 和 accept 阶段。  </p><ul><li>prepare 阶段<ul><li>Proposer提案者：负责提出 proposal，每个提案者在提出提案时都会首先获取到一个 具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。  </li><li>Acceptor表决者：每个表决者在 accept 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案，其编号假设为 maxN。每个表决者仅会 accept 编号大于自己本地 maxN 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 Proposer。<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_1.png"></li></ul></li><li>accept 阶段<br>当一个提案被 Proposer 提出后，如果 Proposer 收到了超过半数的 Acceptor 的批准（Proposer 本身同意），那么此时 Proposer 会给所有的 Acceptor 发送真正的提案（你可以理解为第一阶段为试探），这个时候 Proposer 就会发送提案的内容和提案编号。<br>表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 大于等于 已经批准过的最大提案编号，那么就 accept 该提案（此时执行提案内容但不提交），随后将情况返回给 Proposer 。如果不满足则不回应或者返回 NO 。<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_2.png"><br>当 Proposer 收到超过半数的 accept ，那么它这个时候会向所有的 acceptor 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 acceptor 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要向未批准的 acceptor 发送提案内容和提案编号并让它无条件执行和提交，而对于前面已经批准过该提案的 acceptor 来说 仅仅需要发送该提案的编号 ，让 acceptor 执行提交就行了。<br>而如果 Proposer 如果没有收到超过半数的 accept 那么它将会将 递增 该 Proposal 的编号，然后 重新进入 Prepare 阶段 。<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_3.png" alt="img.png"><br>而如果 Proposer 如果没有收到超过半数的 accept 那么它将会将 递增 该 Proposal 的编号，然后 重新进入 Prepare 阶段 。</li></ul><h4 id="Paxos算法的死循环问题"><a href="#Paxos算法的死循环问题" class="headerlink" title="Paxos算法的死循环问题"></a>Paxos算法的死循环问题</h4><p>其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。<br>比如说，此时提案者 P1 提出一个方案 M1，完成了 Prepare 阶段的工作，这个时候 acceptor 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 Prepare 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 acceptor 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 Prepare 阶段，然后 acceptor ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 Prepare 阶段。<br>就这样无休无止的永远提案下去，这就是 paxos 算法的死循环问题。<br>那么如何解决呢？很简单，人多了容易吵架，我现在 就允许一个能提案 就行了。</p><h3 id="Raft-算法"><a href="#Raft-算法" class="headerlink" title="Raft 算法"></a>Raft 算法</h3><h2 id="分布式事务解决方案"><a href="#分布式事务解决方案" class="headerlink" title="分布式事务解决方案"></a>分布式事务解决方案</h2><h3 id="XA规范-协议"><a href="#XA规范-协议" class="headerlink" title="XA规范/协议"></a>XA规范/协议</h3><p>X/Open组织（现在的Open Group）定义了一套DTP（Distributed Transaction Processing）分布式事务处理模型，主要包含以下四部分：</p><ul><li>AP（应用程序） </li><li>TM（事务管理器）：交易中间件</li><li>RM（资源管理器）：数据库</li><li>CRM（通信资源管理器）：消息中间件</li></ul><p><strong>XA规范</strong>则是DTP模型定义TM和RM之间通讯的接口规范。  </p><p>XA接口函数由数据库厂商提供。<br>TM用它来通知数据库事务的开始、结束、提交、回滚。<br>基于XA规范衍生出下面的二阶段提交（2PC）、三阶段提交（3PC）。  </p><p>XA规范包括两套函数，以xa_开头的及以ax_开头的。<br>以下的函数使事务管理器可以对资源管理器进行的操作：</p><ul><li>xa_open,xa_close：建立和关闭与资源管理器的连接。</li><li>xa_start,xa_end：开始和结束一个本地事务。</li><li>xa_prepare,xa_commit,xa_rollback：预提交、提交、回滚一个本地事务。</li><li>xa_recover：回滚一个已进行预提交的事务。</li><li>ax_开头的函数使资源管理器可以动态地在事务管理器中进行注册，并可以对XID(TRANSACTION IDS)进行操作。</li><li>ax_reg,ax_unreg；允许一个资源管理器在一个TMS(TRANSACTION MANAGER SERVER)中动态注册或撤消注册。<br>XA的一些问题：</li><li>性能（阻塞、响应时间增加、死锁）；</li><li>依赖于独立的J2EE中间件，Weblogic、Jboss，后期轻量级的Atomikos、Narayana、Bitronix；</li><li>不是所有资源(RM)都支持XA协议；</li></ul><h4 id="JTA（Java-Transaction-API）"><a href="#JTA（Java-Transaction-API）" class="headerlink" title="JTA（Java Transaction API）"></a>JTA（Java Transaction API）</h4><p>即Java的事务API，基于XA实现，也就是RM需要支持XA，所以也有JTA(XA)的说法，JTA仅定义了接口。主要包括javax.sql.XADataResource、javax.sql.XAConnection、javax.sql.XAException、javax.transaction.xa.XAResource、javax.transaction.Xid。 目下JTA的实现有几种形式：</p><ul><li>J2EE容器提供的JTA实现（Weblogic、JBoss ）；</li><li>JOTM（Java Open Transaction Manager）、Atomikos，可独立于J2EE容器的环境下实现JTA；</li></ul><h4 id="二阶段提交（2PC）"><a href="#二阶段提交（2PC）" class="headerlink" title="二阶段提交（2PC）"></a>二阶段提交（2PC）</h4><p>2PC就是分布式事务中将事务分为两步进行提交。基于数据库的XA协议完成事务本质上就是二阶段提交（XA、JTA/JTS）。</p><ul><li><p>协调者（Coordinater）：事务管理器（TM）</p></li><li><p>参与者（participants）：资源管理器（RM）<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_4.png"></p></li><li><p><strong>准备阶段</strong>：  </p><ul><li>协调者向参与者发送prepare信息，以询问参与者是否能够提交事务；</li><li>参与者在收到prepare信息后，进行本地事务的预处理，但不提交。并根据处理结果返回，失败not commit or 成功ready ；</li></ul></li><li><p><strong>提交阶段</strong>：  </p><ul><li>如协调者收到参与者的失败消息，则向每个参与者发送rollback消息进行回滚；</li><li>所有参与者都返回ready，则向每个参与者发送提交commit消息，通知参与者进行事务提交；</li></ul></li></ul><p>两阶段提交的一些问题:</p><ul><li>同步阻塞，事务执行过程中所有参与者都是阻塞型的，第三方参与者访问参与者占有的资源时会被阻塞；</li><li>单点故障，协调者一旦发生故障，参与者会被阻塞。尤其在提交阶段，所有参与者都处于锁定资源状态中，无法完成事务操作；（可以选择新的协调者，但无法解决参与者被阻塞的问题）；</li><li>数据不一致，提交阶段协调者向参与者发送commit信息，发生局部网络故障，会导致存在参与者未收到commit信息无法提交事务情况，导致出现数据不一致现象；</li></ul><h4 id="三阶段提交（3PC）"><a href="#三阶段提交（3PC）" class="headerlink" title="三阶段提交（3PC）"></a>三阶段提交（3PC）</h4><p>相比于2PC，3PC把2PC的准备阶段再次进行拆分，并且3PC引入了参与者超时机制。</p><ul><li>canCommit：协调者询问参与者，是否具备执行事务的条件，参与者进行自身事务必要条件的检查；</li><li>preCommit：协调者通知参与者进行事务的预提交；</li><li>doCommit：协调者根据preCommit阶段参与者的反馈结果通知参与者是否进行事务提交或是进行事务回滚。<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_5.png"></li></ul><h4 id="TCC事务补偿方案"><a href="#TCC事务补偿方案" class="headerlink" title="TCC事务补偿方案"></a>TCC事务补偿方案</h4><p>TCC的核心思想就是校验、资源锁定、补偿，对每个操作（Try）都提供确认（Confirm）和取消（cancel）的操作，这样根据操作的结果，来确认是进行Confirm还是Cancel。<br>可以看出XA的两阶段提交是基于资源层面的，而TCC也是一种两阶段提交，但它是基于应用层面的。</p><ul><li>Try：主要负责对业务进行数据检查和资源预留，例如：对资金进行冻结；对状态更改为处理中；</li><li>Confirm：确认执行业务的操作，例如：进行实际资金扣除；更改状态为最终结果；</li><li>Cancel：取消执行业务的操作，例如：解冻资金；更改状态为未处理；</li></ul><p>TCC存在的一些问题：</p><ul><li>业务操作的是不同服务的Try来进行资源预留，每个Try都是独立完成本地事务，因此不会对资源一直加锁。</li><li>业务服务需要提供try、confirm、cancel，业务侵入性强，如不适用三方框架要做到对各阶段状态的感知，比较麻烦。</li><li>Confirm/Cancel要做幂等性设计。</li></ul><p>常用TCC框架：<br>tcc-transaction、ByteTCC、spring-cloud-rest-tcc、Himly</p><p>常见的微服务系统大部分接口调用是同步的，这时候使用TCC来保证一致性是比较合适的。</p><h4 id="SAGA"><a href="#SAGA" class="headerlink" title="SAGA"></a>SAGA</h4><p>Saga的核心是补偿，与TCC不同的是Saga不需要Try，而是直接进行confirm、cancel操作。  </p><ul><li>Confirm：依次按顺序依次执行资源操作，各个资源直接处理本地事务，如无问题，二阶段什么都不用做；</li><li>Cancel：异常情况下需要调用的补偿事务（逆操作）来保证数据的一致性。</li></ul><p>可以看出，Saga和TCC有些类似，都是补偿型事务</p><p>优势：</p><ul><li>一阶段提交本地事务，无锁，高性能；</li><li>事件驱动模式，参与者可异步执行，高吞吐；</li><li>应用成本低，补偿服务易于实现；</li></ul><p>劣势：</p><ul><li>无法保证隔离性（脏写）</li></ul><h4 id="事务消息"><a href="#事务消息" class="headerlink" title="事务消息"></a>事务消息</h4><p>有一些情况，服务间调用时异步的，服务A将消息发送到MQ，服务B进行消息的消费。这时我们就需要用到可靠消息最终一致性来解决分布式事务问题</p><ul><li>可靠消息：即这个消息一定是可靠的，并且最终一定需要被消费的。 </li><li>最终一致性：过程中数据存在一定时间内的不一致，但超过限定时间后，需要最终会保持一致。</li></ul><p>保证以上两点的情况下，可以通过消息中间件（RocketMQ）来完成分布式事务处理，因为RocketMQ支持事务消息，可以方便的让我们进行分布式事务控制。</p><p>RocketMQ的事务消息的原理：<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/img_6.png" alt="img.png"></p><p>half message：半消息，此时消息不能被consumer所发现和消费，需producer进行二次消息确认。</p><ul><li>producer发送half message给MQ Server；</li><li>producer根据MQ Server应答结果判断half message是否发送成功；</li><li>producer处理本地事务；</li><li>producer发送最终确认消息commit / rollback；</li><li>commit：consumer对消息可见并进行消费；</li><li>rollback：discard抛弃消息，consumer无法进行消息消费；</li></ul><p>如遇异常情况下step4最终确认消息为达到MQ Server，MQ Server会定期查询当前处于半消息状态下的消息，主动进行消息回查来询问producer该消息的最终状态；</p><ul><li>producer检查本地事务执行的最终结果；</li><li>producer根据检查到的结果，再次提交确认消息，MQ Server仍然按照step4进行后续操作。</li></ul><p>事务消息发送对应步骤1、2、3、4，事务消息回查对应步骤5、6、7。<br>由以上步骤可以看出通过事务性消息的两步操作，避免了消息直接投递所产生一些问题。最终投递到MQ Server的消息，是真实可靠且必须被消费的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="分布式事务设计权衡点"><a href="#分布式事务设计权衡点" class="headerlink" title="分布式事务设计权衡点"></a>分布式事务设计权衡点</h3><ul><li>实现复杂度：事务模式与当前业务结合，实施成本是否过高；</li><li>业务侵入性：基于注解、XML、补偿逻辑； </li><li>TC/TM部署：独立部署、与应用部署；</li><li>性能：回滚概率、回滚所付出的代价、响应时间、吞吐量；</li><li>高可用：数据库、注册中心、配置中心</li><li>持久化：文件、数据库；</li><li>同步/异步：分布式事务执行过程中是否阻塞，还是非阻塞；</li></ul><h3 id="分布式事务解决方案对比"><a href="#分布式事务解决方案对比" class="headerlink" title="分布式事务解决方案对比"></a>分布式事务解决方案对比</h3><p>分布式系统中，基于不同的一致性需求产生了不同的分布式事务解决方案，追求强一致的两阶段提交、追求最终一致性的柔性事务和事务消息等等。  </p><p>我们综合对比下几种分布式事务解决方案：  </p><ul><li>一致性保证：XA &gt; TCC = SAGA &gt; 事务消息  </li><li>业务友好性：XA &gt; 事务消息 &gt; SAGA &gt; TCC  </li><li>性 能 损 耗：XA &gt; TCC &gt; SAGA = 事务消息</li></ul><p>在柔性事务解决方案中，虽然SAGA和TCC看上去可以保证数据的最终一致性，但分布式系统的生产环境复杂多变，某些情况是可以导致柔性事务机制失效的，所以无论使用那种方案，都需要最终的兜底策略，人工校验，修复数据。</p><h3 id="分布式事务框架Seata"><a href="#分布式事务框架Seata" class="headerlink" title="分布式事务框架Seata"></a>分布式事务框架Seata</h3><p>阿里开源的Seata 是一款分布式事务解决方案，提供了 AT、TCC、SAGA 和 XA 事务模式。</p><p>Seata架构的亮点主要有几个:</p><ul><li>应用层基于SQL解析实现了自动补偿，从而最大程度的降低业务侵入性；</li><li>将分布式事务中TC（事务协调者）独立部署，负责事务的注册、回滚（支持多种注册中心形式以及本地文件形式）；</li><li>通过全局锁实现了写隔离与读隔离。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是分布式事务&quot;&gt;&lt;a href=&quot;#什么是分布式事务&quot; class=&quot;headerlink&quot; title=&quot;什么是分布式事务&quot;&gt;&lt;/a&gt;什么是分布式事务&lt;/h2&gt;&lt;p&gt;分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式</summary>
      
    
    
    
    
    <category term="分布式系统" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统02之分布式ID解决方案</title>
    <link href="http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8FID%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8FID%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2021-07-09T10:32:44.000Z</published>
    <updated>2021-07-12T03:32:47.564Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么需要分布式ID"><a href="#为什么需要分布式ID" class="headerlink" title="为什么需要分布式ID"></a>为什么需要分布式ID</h2><p>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。比如数据量太大之后，往往需要对进行对数据进行分库分表，分库分表后需要有一个唯一 ID 来标识一条数据或消息，数据库的自增 ID 显然不能满足需求。</p><h2 id="分布式ID生成方案"><a href="#分布式ID生成方案" class="headerlink" title="分布式ID生成方案"></a>分布式ID生成方案</h2><p><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8FID%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/img_1.png"></p><h3 id="数据库自增ID"><a href="#数据库自增ID" class="headerlink" title="数据库自增ID"></a>数据库自增ID</h3><p>需要一个单独的Mysql实例，虽然可行，但是基于性能与可靠性来考虑的话都不够，业务系统每次需要一个ID时，都需要请求数据库获取，性能低，并且如果此数据库实例下线了，那么将影响所有的业务系统。</p><h3 id="数据库多主模式"><a href="#数据库多主模式" class="headerlink" title="数据库多主模式"></a>数据库多主模式</h3><p>多个数据库主节点实例，单独设置步长防止产生相同ID，或者使用号段模式每个节点生产部分号段的ID</p><h3 id="雪花算法"><a href="#雪花算法" class="headerlink" title="雪花算法"></a>雪花算法</h3><p>核心思想是：分布式ID固定是一个long型的数字，一个long型占8个字节，也就是64个bit，原始snowflake算法中对于bit的分配如下图：<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8FID%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/img.png" alt="img.png"></p><ul><li>第一个bit位是标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以固定为0。</li><li>时间戳部分占41bit，这个是毫秒级的时间，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间），这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，(1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69年</li><li>工作机器id占10bit，这里比较灵活，比如，可以使用前5位作为数据中心机房标识，后5位作为单机房机器标识，可以部署1024个节点。</li><li>序列号部分占12bit，支持同一毫秒内同一个节点可以生成4096个ID</li><li>备注： 工作机器ID可以通过某种改造自动生成。</li></ul><h3 id="Redis自增ID"><a href="#Redis自增ID" class="headerlink" title="Redis自增ID"></a>Redis自增ID</h3><p>使用Redis来生成分布式ID，其实和利用Mysql自增ID类似，可以利用Redis中的incr命令来实现原子性的自增与返回，比如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set seq_id 1 // 初始化自增ID为1 OK </span><br><span class="line">127.0.0.1:6379&gt; incr seq_id // 增加1，并返回 (integer) 2 </span><br><span class="line">127.0.0.1:6379&gt; incr seq_id // 增加1，并返回</span><br><span class="line">备注：使用redis需要考虑持久化问题,RDB&amp;AOF。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;为什么需要分布式ID&quot;&gt;&lt;a href=&quot;#为什么需要分布式ID&quot; class=&quot;headerlink&quot; title=&quot;为什么需要分布式ID&quot;&gt;&lt;/a&gt;为什么需要分布式ID&lt;/h2&gt;&lt;p&gt;在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。比如数据量太大之</summary>
      
    
    
    
    
    <category term="分布式系统" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统01之分布式系统理论</title>
    <link href="http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/"/>
    <id>http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/</id>
    <published>2021-07-09T10:32:14.000Z</published>
    <updated>2021-07-12T03:06:03.203Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h2><h3 id="名词解析"><a href="#名词解析" class="headerlink" title="名词解析"></a>名词解析</h3><p>CAP理论作为分布式系统的基础理论,它描述的是一个分布式系统在以下三个特性中：</p><ul><li><strong>一致性（Consistency）</strong><ul><li>所有节点访问同一份最新的数据副本</li></ul></li><li><strong>可用性（Availability）</strong><ul><li>非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。</li></ul></li><li><strong>分区容错性（Partition tolerance）</strong><ul><li>分布式系统出现网络分区（多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域）的时候，仍然能够对外提供服务。<br>最多满足其中的两个特性。 </li></ul></li></ul><p>也就是下图所描述的。分布式系统要么满足CA,要么CP，要么AP。无法同时满足CAP。<br><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA/img.png"></p><h3 id="CAP三者不可兼得，该如何取舍："><a href="#CAP三者不可兼得，该如何取舍：" class="headerlink" title="CAP三者不可兼得，该如何取舍："></a>CAP三者不可兼得，该如何取舍：</h3><ul><li><strong>CA</strong>: 优先保证一致性和可用性，放弃分区容错。 这也意味着放弃系统的扩展性，系统不再是分布式的，有违设计的初衷。<ul><li>当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。</li><li><strong>因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</strong></li></ul></li><li><strong>CP</strong>: 优先保证一致性和分区容错性，放弃可用性。 在<strong>数据一致性要求比较高的场合</strong>(譬如:zookeeper,Hbase) 是比较常见的做法，一旦发生网络故障或者消息丢失，就会牺牲用户体验，等恢复之后用户才逐渐能访问。</li><li><strong>AP</strong>: 优先保证可用性和分区容错性，放弃一致性。 NoSQL中的Cassandra 就是这种架构。跟CP一样，放弃一致性不是说一致性就不保证了，而是逐渐的变得一致。</li></ul><h3 id="实际应用案例–注册中心"><a href="#实际应用案例–注册中心" class="headerlink" title="实际应用案例–注册中心"></a>实际应用案例–注册中心</h3><p>常见的可以作为注册中心的组件有：ZooKeeper、Eureka、Nacos…。 </p><ul><li>ZooKeeper 保证的是 CP。<br>任何时刻对 ZooKeeper 的读请求都能得到一致性的结果。<br>但是， ZooKeeper 不保证每次请求的可用性，比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。</li><li>Eureka 保证的则是 AP。<br>Eureka 在设计的时候就是优先保证 A （可用性）。<br>在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。<br>因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。<br>Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。   </li><li>Nacos 不仅支持 CP 也支持 AP。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等。<br>如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”（网络连接通信正常）的话，我们要思考如何保证 CA。</p><h2 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h2><h3 id="BASE理论名词解析"><a href="#BASE理论名词解析" class="headerlink" title="BASE理论名词解析"></a>BASE理论名词解析</h3><ul><li><strong>基本可用（Basically Available）</strong><br>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。<br>什么叫允许损失部分可用性呢？ <ul><li>响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 </li><li>系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。</li></ul></li><li><strong>软状态（Soft State）</strong><br>软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</li><li><strong>最终一致性（Eventually Consistent）</strong><br>虽然允许软状态，但是系统不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。<br>实际工程实践中，最终一致性分为5种：<ul><li>因果一致性（Causal consistency）<br>如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。于此同时，和节点A无因果关系的节点C的数据访问则没有这样的限制。</li><li>读己之所写（Read your writes）<br>节点A更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。</li><li>会话一致性（Session consistency）<br>系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。</li><li>单调读一致性（Monotonic read consistency）<br>如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。</li><li>单调写一致性（Monotonic write consistency）<br>一个系统要能够保证来自同一个节点的写操作被顺序的执行。</li></ul></li></ul><h3 id="系统一致性说明"><a href="#系统一致性说明" class="headerlink" title="系统一致性说明"></a>系统一致性说明</h3><ul><li><strong>强一致性</strong>：系统写入了什么，读出来的就是什么。 </li><li><strong>弱一致性</strong>：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。</li><li><strong>最终一致性</strong>：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。</li></ul><h3 id="BASE理论的核心思想"><a href="#BASE理论的核心思想" class="headerlink" title="BASE理论的核心思想"></a>BASE理论的核心思想</h3><p>BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。</span><br></pre></td></tr></table></figure><p>CAP的3选2实际是个伪命题，实际上，系统没有发生P(分区)的话，必须在C（一致性）和A（可用性）之间任选其一。<br>分区的情况很少出现，CAP在大多时间能够同时满足C和A。<br>对于分区存在或者探知其影响的情况下，需要提供一种预备策略做出处理：</p><ul><li>探知分区的发生；</li><li>进入显示的分区模式，限制某些操作；</li><li>启动恢复过程，恢复数据一致性，补偿分区发生期间的错误。</li></ul><p>因此，AP方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。<br>在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。</p><h2 id="ACID本地事务四大特性"><a href="#ACID本地事务四大特性" class="headerlink" title="ACID本地事务四大特性"></a>ACID本地事务四大特性</h2><ul><li><strong>原子性（atomicity）</strong><br>一个事务中的所有操作，不可分割，要么全部成功，要么全部失败；</li><li><strong>一致性（consistency）</strong><br>一个事务执行前与执行后数据的完整性必须保持一致；</li><li><strong>隔离性（isolation）</strong><br>一个事务的执行，不能被其他事务干扰，多并发时事务之间要相互隔离；</li><li><strong>持久性（durability）</strong><br>一个事务一旦被提交，它对数据库中数据的改变是永久性的。</li></ul><h2 id="幂等性设计"><a href="#幂等性设计" class="headerlink" title="幂等性设计"></a>幂等性设计</h2><p>幂等（Idempotent）是一个数学与计算机学中的概念。f(n) = 1^n // 无论n等于多少，f(n)永远值等于1；在程序中，使用相同参数执行同一个方法，每一次执行结果都是相同的，即具有幂等性。</p><h1 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h1><ul><li>ACID 是数据库事务完整性的理论，</li><li>CAP 是分布式系统设计理论，</li><li>BASE 是 CAP 理论中 AP 方案的延伸。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;CAP理论&quot;&gt;&lt;a href=&quot;#CAP理论&quot; class=&quot;headerlink&quot; title=&quot;CAP理论&quot;&gt;&lt;/a&gt;CAP理论&lt;/h2&gt;&lt;h3 id=&quot;名词解析&quot;&gt;&lt;a href=&quot;#名词解析&quot; class=&quot;headerlink&quot; title=&quot;名词解析&quot;</summary>
      
    
    
    
    
    <category term="分布式系统" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统00之什么是分布式系统</title>
    <link href="http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    <id>http://example.com/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</id>
    <published>2021-07-09T10:31:51.000Z</published>
    <updated>2021-07-12T08:53:12.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是分布式系统"><a href="#什么是分布式系统" class="headerlink" title="什么是分布式系统"></a>什么是分布式系统</h2><p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。</p><p>分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是<strong>利用更多的机器，处理更多的数据</strong>。</p><p>首先需要明确的是，只有</p><ul><li>单个节点的处理能力无法满足日益增长的计算、存储任务</li><li>且硬件的提升（加内存、加磁盘、使用更好的CPU）高昂到得不偿失</li><li>应用程序也不能进一步优化  </li></ul><p>我们才需要考虑分布式系统。</p><p>因为，分布式系统要解决的问题本身就是和单机系统一样的，而由于分布式系统多节点、通过网络通信的拓扑结构，会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。</p><p>分布式系统怎么将任务分发到这些计算机节点呢，很简单的思想，分而治之，即分片（partition）。对于计算，那么就是对计算任务进行切换，每个节点算一些，最终汇总就行了，这就是MapReduce的思想；对于存储，更好理解一下，每个节点存一部分数据就行了。当数据规模变大的时候，Partition是唯一的选择，同时也会带来一些好处：</p><ul><li>提升性能和并发，操作被分发到不同的分片，相互独立</li><li>提升系统的可用性，即使部分分片不能用，其他分片不会受到影响</li></ul><p>理想的情况下，有分片就行了，但事实的情况却不大理想。</p><p>原因在于，分布式系统中有大量的节点，且通过网络通信。单个节点的故障（进程crash、断电、磁盘损坏）是个小概率事件，但整个系统的故障率会随节点的增加而指数级增加，网络通信也可能出现断网、高延迟的情况。在这种一定会出现的“异常”情况下，分布式系统还是需要继续稳定的对外提供服务，即需要较强的容错性。最简单的办法，就是冗余或者复制集（Replication），即多个节点负责同一个任务，最为常见的就是分布式存储中，多个节点复杂存储同一份数据，以此增强可用性与可靠性。同时，Replication也会带来性能的提升，比如数据的locality可以减少用户的等待时间。</p><p><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/img.png"></p><p>Partition和Replication是解决分布式系统问题的一记组合拳，很多具体的问题都可以用这个思路去解决。但这并不是银弹，往往是为了解决一个问题，会引入更多的问题，比如为了可用性与可靠性保证，引用了冗余（复制集）。有了冗余，各个副本间的一致性问题就变得很头疼，一致性在系统的角度和用户的角度又有不同的等级划分。如果要保证强一致性，那么会影响可用性与性能，在一些应用（比如电商、搜索）是难以接受的。如果是最终一致性，那么就需要处理数据冲突的情况。CAP、FLP这些理论告诉我们，在分布式系统中，没有最佳的选择，都是需要权衡，做出最合适的选择。</p><h2 id="分布式系统面临的挑战"><a href="#分布式系统面临的挑战" class="headerlink" title="分布式系统面临的挑战"></a>分布式系统面临的挑战</h2><ul><li><p>异构的机器与网络：</p><p>  分布式系统中的机器，配置不一样，其上运行的服务也可能由不同的语言、架构实现，因此处理能力也不一样；节点间通过网络连接，而不同网络运营商提供的网络的带宽、延时、丢包率又不一样。怎么保证大家齐头并进，共同完成目标，这四个不小的挑战。</p></li><li><p>普遍的节点故障：</p><p>  虽然单个节点的故障概率较低，但节点数目达到一定规模，出故障的概率就变高了。分布式系统需要保证故障发生的时候，系统仍然是可用的，这就需要监控节点的状态，在节点故障的情况下将该节点负责的计算、存储任务转移到其他节点</p></li><li><p>不可靠的网络：</p><p>  节点间通过网络通信，而网络是不可靠的。可能的网络问题包括：网络分割、延时、丢包、乱序。</p><p>  相比单机过程调用，网络通信最让人头疼的是超时：节点A向节点B发出请求，在约定的时间内没有收到节点B的响应，那么B是否处理了请求，这个是不确定的，这个不确定会带来诸多问题，最简单的，是否要重试请求，节点B会不会多次处理同一个请求。</p></li></ul><p>总而言之，分布式的挑战来自<strong>不确定性</strong>，不确定计算机什么时候crash、断电，不确定磁盘什么时候损坏，不确定每次网络通信要延迟多久，也不确定通信对端是否处理了发送的消息。而分布式的规模放大了这个不确定性，不确定性是令人讨厌的，所以有诸多的分布式理论、协议来保证在这种不确定性的情况下，系统还能继续正常工作。</p><h2 id="分布式系统特性与衡量标准"><a href="#分布式系统特性与衡量标准" class="headerlink" title="分布式系统特性与衡量标准"></a>分布式系统特性与衡量标准</h2><ul><li><p><strong>透明性</strong><br>使用分布式系统的用户并不关心系统是怎么实现的，也不关心读到的数据来自哪个节点，对用户而言，分布式系统的最高境界是用户根本感知不到这是一个分布式系统。</p></li><li><p><strong>可扩展性</strong><br>分布式系统的根本目标就是为了处理单个计算机无法处理的任务，当任务增加的时候，分布式系统的处理能力需要随之增加。简单来说，要比较方便的通过增加机器来应对数据量的增长，同时，当任务规模缩减的时候，可以撤掉一些多余的机器，达到动态伸缩的效果</p></li><li><p><strong>可用性与可靠性</strong><br>一般来说，分布式系统是需要长时间甚至7*24小时提供服务的。可用性是指系统在各种情况对外提供服务的能力，简单来说，可以通过不可用时间与正常服务时间的必知来衡量；而可靠性而是指计算结果正确、存储的数据不丢失。</p></li><li><p><strong>高性能</strong><br>不管是单机还是分布式系统，大家都非常关注性能。不同的系统对性能的衡量指标是不同的，最常见的：高并发，单位时间内处理的任务越多越好；低延迟：每个任务的平均时间越少越好。这个其实跟操作系统CPU的调度策略很像</p></li><li><p><strong>一致性</strong><br>分布式系统为了提高可用性可靠性，一般会引入冗余（复制集）。那么如何保证这些节点上的状态一致，这就是分布式系统不得不面对的一致性问题。一致性有很多等级，一致性越强，对用户越友好，但会制约系统的可用性；一致性等级越低，用户就需要兼容数据不一致的情况，但系统的可用性、并发性很高很多。</p></li></ul><h2 id="一个简化的架构图"><a href="#一个简化的架构图" class="headerlink" title="一个简化的架构图"></a>一个简化的架构图</h2><p><img src="/2021/07/09/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/img_2.png" alt="img.png"></p><h3 id="概念及其实现"><a href="#概念及其实现" class="headerlink" title="概念及其实现"></a>概念及其实现</h3><p>负载均衡：<br>Nginx：高性能、高并发的web服务器；功能包括负载均衡、反向代理、静态内容缓存、访问控制；工作在应用层</p><p>LVS： Linux virtual server，基于集群技术和Linux操作系统实现一个高性能、高可用的服务器；工作在网络层</p><p>webserver：<br>Java：Tomcat，Apache，Jboss</p><p>Python：gunicorn、uwsgi、twisted、webpy、tornado</p><p>service：<br>SOA、微服务、spring boot，django</p><p>容器：<br>docker，kubernetes</p><p>cache：<br>memcache、redis等</p><p>协调中心：<br>zookeeper、etcd等</p><p>zookeeper使用了Paxos协议Paxos是强一致性，高可用的去中心化分布式。zookeeper的使用场景非常广泛，之后细讲。</p><p>rpc框架：<br>grpc、dubbo、brpc</p><p>dubbo是阿里开源的Java语言开发的高性能RPC框架，在阿里系的诸多架构中，都使用了dubbo + spring boot</p><p>消息队列：<br>kafka、rabbitMQ、rocketMQ、QSP</p><p>消息队列的应用场景：异步处理、应用解耦、流量削锋和消息通讯</p><p>实时数据平台：<br>storm、akka</p><p>离线数据平台：<br>hadoop、spark</p><p>PS: spark、akka、kafka都是scala语言写的，看到这个语言还是很牛逼的</p><p>dbproxy：<br>cobar也是阿里开源的，在阿里系中使用也非常广泛，是关系型数据库的sharding + replica 代理</p><p>db：<br>mysql、oracle、MongoDB、HBase</p><p>搜索：<br>elasticsearch、solr</p><p>日志：<br>rsyslog、elk、flume</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是分布式系统&quot;&gt;&lt;a href=&quot;#什么是分布式系统&quot; class=&quot;headerlink&quot; title=&quot;什么是分布式系统&quot;&gt;&lt;/a&gt;什么是分布式系统&lt;/h2&gt;&lt;p&gt;分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。&lt;/</summary>
      
    
    
    
    
    <category term="分布式系统" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>高并发系统03之高并发三大利器之降级</title>
    <link href="http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%8D%E7%BA%A7/"/>
    <id>http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F03%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%8D%E7%BA%A7/</id>
    <published>2021-07-09T08:28:03.000Z</published>
    <updated>2021-07-09T09:17:28.380Z</updated>
    
    <content type="html"><![CDATA[<p><strong>高并发三大利器</strong></p><ul><li>缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹</li><li>降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再打开</li><li>限流  –  通过对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务（定向到错误页或告知资源没有了）、排队或等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据，如商品详情页库存默认有货）、特权处理(优先处理需要高保障的用户群体)</li></ul><h3 id="什么是服务降级"><a href="#什么是服务降级" class="headerlink" title="什么是服务降级"></a>什么是服务降级</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。</span><br></pre></td></tr></table></figure><p>服务降级主要用于当整个微服务架构整体的负载超出了预设的上限阈值或即将到来的流量预计将会超过预设的阈值时，为了保证重要或基本的服务能正常运行，将一些 不重要 或 不紧急 的服务或任务进行服务的 <strong>延迟使用</strong> 或 <strong>暂停使用</strong>。</p><p>降级就是为了解决资源不足和访问量增加的矛盾。</p><h3 id="服务降级方式"><a href="#服务降级方式" class="headerlink" title="服务降级方式"></a>服务降级方式</h3><ul><li><strong>延迟服务</strong>：定时任务处理、或者mq延时处理。比如新用户注册送多少优惠券可以提示用户优惠券会24小时到达用户账号中，我们可以选择再凌晨流量较小的时候，批量去执行送券</li><li><strong>页面降级</strong>：页面点击按钮全部置灰，或者页面调整成为一个静态页面显示“系统正在维护中，。。。。”。</li><li><strong>关闭非核心服务</strong>：比如电商关闭推荐服务、关闭运费险、退货退款等。保证主流程的核心服务下单付款就好。</li><li><strong>写降级</strong>：比如秒杀抢购，我们可以只进行Cache的更新返回，然后通过mq异步扣减库存到DB，保证最终一致性即可，此时可以将DB降级为Cache。</li><li><strong>读降级</strong>：比如多级缓存模式，如果后端服务有问题，可以降级为只读缓存，这种方式适用于对读一致性要求不高的场景。</li></ul><h3 id="服务熔断"><a href="#服务熔断" class="headerlink" title="服务熔断"></a>服务熔断</h3><h4 id="服务雪崩"><a href="#服务雪崩" class="headerlink" title="服务雪崩"></a>服务雪崩</h4><p>多个微服务之间调用的时候，比如A服务调用了B服务，B服务调用了C服务，然后C服务由于机器宕机或者网略故障， 然后就会导致B服务调用C服务的时候超时，然后A服务调用B服务也会超时，最终整个链路都不可用了，导致整个系统不可用就跟雪蹦一样。</p><h4 id="雪崩效应产生的几种场景"><a href="#雪崩效应产生的几种场景" class="headerlink" title="雪崩效应产生的几种场景"></a>雪崩效应产生的几种场景</h4><p><strong>突增流量</strong>：比如一大波爬虫，或者黑客攻击等。<br><strong>程序bug</strong>：代码死循环，或者资源未释放等。<br><strong>硬件原因</strong>：机器宕机、机房断电、光纤被挖断等。  </p><h4 id="服务熔断-1"><a href="#服务熔断-1" class="headerlink" title="服务熔断"></a>服务熔断</h4><p>熔断机制是应对雪崩效应的一种微服务链路保护机制，在互联网系统中当下游的服务因为某种原因突然变得不可用或响应过慢，上游服务为了保证自己整体服务的可用性，暂时不再继续调用目标服务，直接快速返回，快速释放资源。如果目标服务情况好转则恢复调用。</p><h3 id="熔断和降级的比较"><a href="#熔断和降级的比较" class="headerlink" title="熔断和降级的比较"></a>熔断和降级的比较</h3><h4 id="共性"><a href="#共性" class="headerlink" title="共性"></a>共性</h4><ul><li>目的很一致：都是从可用性可靠性着想，为防止系统的整体缓慢甚至崩溃，采用的技术手段，都是为了保证系统的稳定。</li><li>最终表现类似:对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用；</li><li>粒度一般都是服务级别:当然，业界也有不少更细粒度的做法，比如做到数据持久层（允许查询，不允许增删改）；</li><li>自治性要求很高: 熔断模式一般都是服务基于策略的自动触发，比如</li><li>降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段；</li></ul><h4 id="差异性"><a href="#差异性" class="headerlink" title="差异性"></a>差异性</h4><ul><li>触发原因不太一样，服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑；</li><li>管理目标的层次不太一样，熔断其实是一个框架级的处理，每个微服务都需要（无层级之分），而降级一般需要对业务有层级之分（比如降级一般是从最外围服务开始）熔断是降级方式的一种体现。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;高并发三大利器&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹&lt;/li&gt;
&lt;li&gt;降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再</summary>
      
    
    
    
    
    <category term="高并发系统" scheme="http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>高并发系统02之高并发三大利器之缓存</title>
    <link href="http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E7%BC%93%E5%AD%98/"/>
    <id>http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E7%BC%93%E5%AD%98/</id>
    <published>2021-07-09T08:27:12.000Z</published>
    <updated>2021-07-09T10:04:35.365Z</updated>
    
    <content type="html"><![CDATA[<p><strong>高并发三大利器</strong></p><ul><li>缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹</li><li>降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再打开</li><li>限流  –  通过对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务（定向到错误页或告知资源没有了）、排队或等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据，如商品详情页库存默认有货）、特权处理(优先处理需要高保障的用户群体)</li></ul><h2 id="缓存分类"><a href="#缓存分类" class="headerlink" title="缓存分类"></a>缓存分类</h2><ul><li>分布式缓存： 如redis、memcached等</li><li>本地（进程内）缓存： 如ehcache、GuavaCache、Caffeine等</li></ul><h2 id="缓存特性"><a href="#缓存特性" class="headerlink" title="缓存特性"></a>缓存特性</h2><h3 id="命中率"><a href="#命中率" class="headerlink" title="命中率"></a>命中率</h3><p>命中率=命中数/（命中数+没有命中数）当某个请求能够通过访问缓存而得到响应时，称为缓存命中。缓存命中率越高，缓存的利用率也就越高。</p><h3 id="最大空间"><a href="#最大空间" class="headerlink" title="最大空间"></a>最大空间</h3><p>缓存中可以容纳最大元素的数量。当缓存存放的数据超过最大空间时，就需要根据淘汰算法来淘汰部分数据存放新到达的数据。</p><h3 id="淘汰算法"><a href="#淘汰算法" class="headerlink" title="淘汰算法"></a>淘汰算法</h3><p>缓存的存储空间有限制，当缓存空间被用满时，如何保证在稳定服务的同时有效提升命中率？这就由缓存淘汰算法来处理，设计适合自身数据特征的淘汰算法能够有效提升缓存命中率。<br>常见的淘汰算法有：</p><ul><li><p>FIFO(first in first out)「先进先出」<br>最先进入缓存的数据在缓存空间不够的情况下（超出最大元素限制）会被优先被清除掉，以腾出新的空间接受新的数据。策略算法主要比较缓存元素的创建时间。「适用于保证高频数据有效性场景，优先保障最新数据可用」。</p></li><li><p>LFU(less frequently used)「最少使用」<br>无论是否过期，根据元素的被使用次数判断，清除使用次数较少的元素释放空间。策略算法主要比较元素的hitCount（命中次数）。「适用于保证高频数据有效性场景」。</p></li><li><p>LRU(least recently used)「最近最少使用」<br>无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。策略算法主要比较元素最近一次被get使用时间。「比较适用于热点数据场景，优先保证热点数据的有效性。」</p></li></ul><h2 id="本地缓存"><a href="#本地缓存" class="headerlink" title="本地缓存"></a>本地缓存</h2><p>常见本地缓存有以下几种实现方式：</p><p><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F02%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E7%BC%93%E5%AD%98/img.png"></p><p>其中性能最佳的是Caffeine，了解更详细信息参考： <a href="https://mp.weixin.qq.com/s?__biz=MzIyMjQwMTgyNA==&mid=2247483811&idx=1&sn=9d0b207044b5fe447169d630a7f77aab&scene=21#wechat_redirect">本地缓存性能之王</a></p><h2 id="分布式缓存"><a href="#分布式缓存" class="headerlink" title="分布式缓存"></a>分布式缓存</h2><p>分布式缓存详细信息参考redis系列文章</p><h2 id="缓存更新方案"><a href="#缓存更新方案" class="headerlink" title="缓存更新方案"></a>缓存更新方案</h2><p>我们一般的缓存更新主要有以下几种更新策略：</p><ul><li>先更新缓存，再更新数据库</li><li>先更新数据库，再更新缓存</li><li>先删除缓存，再更新数据库</li><li>先更新数据源库，再删除缓存</li></ul><p>至于选择哪种更新策略的话，没有绝对的选择，可以根据自己的业务情况来选择适合自己的。<br>不过一般推荐的话是选择 「<strong>先更新数据源库，再删除缓存</strong>」。</p><h2 id="缓存常见问题"><a href="#缓存常见问题" class="headerlink" title="缓存常见问题"></a>缓存常见问题</h2><h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h3><p>大量查询数据库不存在数据，缓存无数据，大量无效请求落库。</p><p>解决方案</p><ul><li>数据库不存在数据写空值入缓存</li></ul><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h3><p>大规模缓存崩溃，大量请求落库。</p><p>解决方案</p><ul><li>事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。</li><li>事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。</li><li>事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。</li></ul><h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h3><p>热点key失效瞬间大量请求落库。</p><p>解决方案</p><ul><li>基本不会发生更新的，则可尝试将该热点数据设置为永不过期。</li><li>更新不频繁且更新时间段，加互斥锁保证少量请求重建缓存。</li><li>数据更新频繁或更新时间长，定时线程主动重建缓存。</li></ul><h3 id="缓存双写一致性"><a href="#缓存双写一致性" class="headerlink" title="缓存双写一致性"></a>缓存双写一致性</h3><p>缓存使用方法：<br>读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。<br>更新的时候，先更新数据库，然后再删除缓存。</p><p>不一致解决方案</p><ul><li>初级：先删除缓存，再更新数据库。</li><li>高并发：使用队列做轻异步，多个并发更新请求阻塞过滤。（必须压测防止长时间阻塞积压）</li></ul><h3 id="redis并发竞争"><a href="#redis并发竞争" class="headerlink" title="redis并发竞争"></a>redis并发竞争</h3><p>多客户端同时并发写key，后来的数据先改。</p><p>解决方案：</p><ul><li>Redis存在CAS方案</li><li>实现分布式锁</li><li>写前判断版本号</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;高并发三大利器&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹&lt;/li&gt;
&lt;li&gt;降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再</summary>
      
    
    
    
    
    <category term="高并发系统" scheme="http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>高并发系统01之高并发三大利器之限流</title>
    <link href="http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/"/>
    <id>http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/</id>
    <published>2021-07-09T07:54:02.000Z</published>
    <updated>2021-07-09T09:08:48.060Z</updated>
    
    <content type="html"><![CDATA[<p><strong>高并发三大利器</strong></p><ul><li>缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹</li><li>降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再打开</li><li>限流  –  通过对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务（定向到错误页或告知资源没有了）、排队或等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据，如商品详情页库存默认有货）、特权处理(优先处理需要高保障的用户群体)</li></ul><h2 id="限流算法"><a href="#限流算法" class="headerlink" title="限流算法"></a>限流算法</h2><h3 id="快速失败-–-滑动时间窗口"><a href="#快速失败-–-滑动时间窗口" class="headerlink" title="快速失败 – 滑动时间窗口"></a>快速失败 – 滑动时间窗口</h3><p>滑动窗口算法是将时间周期分为N个小周期，分别记录每个小周期内访问次数，并且根据时间滑动删除过期的小周期<br><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/%E6%BB%91%E5%8A%A8%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3.png"></p><h3 id="排队等待-漏桶算法"><a href="#排队等待-漏桶算法" class="headerlink" title="排队等待 - 漏桶算法"></a>排队等待 - 漏桶算法</h3><p>漏桶算法思路很简单，水（请求）先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。<br><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/%E6%BC%8F%E6%A1%B6%E7%AE%97%E6%B3%95.png" alt="漏桶算法"></p><p>首先，我们有一个固定容量的桶，有水流进来，也有水流出去。对于流进来的水来说，我们无法预计一共有多少水会流进来，也无法预计水流的速度。但是对于流出去的水来说，这个桶可以固定水流出的速率。而且，当桶满了之后，多余的水将会溢出。</p><p>我们将算法中的水换成实际应用中的请求，我们可以看到漏桶算法天生就限制了请求的速度。当使用了漏桶算法，我们可以保证接口会以一个常速速率来处理请求。所以漏桶算法天生不会出现临界问题。<br>漏桶算法可以粗略的认为就是注水漏水过程，往桶中<strong>以一定速率流出水，以任意速率流入水</strong>，当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。</p><h3 id="Warm-Up-令牌桶算法"><a href="#Warm-Up-令牌桶算法" class="headerlink" title="Warm Up - 令牌桶算法"></a>Warm Up - 令牌桶算法</h3><p>首先，我们有一个固定容量的桶，桶里存放着令牌（token）。<br>桶一开始是空的，token以 一个固定的速率r往桶里填充，直到达到桶的容量，多余的令牌将会被丢弃。<br>每当一个请求过来时，就会尝试从桶里移除一个令牌，如果没有令牌的话，请求无法通过。</p><p><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/aegar-6o1hz.png"></p><h4 id="解决问题："><a href="#解决问题：" class="headerlink" title="解决问题："></a>解决问题：</h4><ul><li>Warm Up（冷启动/预热）：通过”冷启动”，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮。</li></ul><h3 id="各算法适用场景"><a href="#各算法适用场景" class="headerlink" title="各算法适用场景"></a>各算法适用场景</h3><ul><li>计数法用于简单粗暴的连接池数量等。</li><li>令牌桶可以用来保护自己，主要用来对调用者频率进行限流，为的是让自己不被打垮。所以如果自己本身有处理能力的时候，如果流量突发（实际消费能力强于配置的流量限制），那么实际处理速率可以超过配置的限制。</li><li>漏桶算法，这是用来保护他人，也就是保护他所调用的系统。主要场景是，当调用的第三方系统本身没有保护机制，或者有流量限制的时候，我们的调用速度不能超过他的限制，由于我们不能更改第三方系统，所以只有在主调方控制。这个时候，即使流量突发，也必须舍弃。因为消费能力是第三方决定的。</li></ul><p>** 简单粗暴场景用计数法。如果要让自己的系统不被打垮，用令牌桶。如果保证别人的系统不被打垮，用漏桶。**</p><h2 id="流量控制组件对比"><a href="#流量控制组件对比" class="headerlink" title="流量控制组件对比"></a>流量控制组件对比</h2><p><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F01%E4%B9%8B%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E5%88%A9%E5%99%A8%E4%B9%8B%E9%99%90%E6%B5%81/aegar-6o1hz.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/xuwc/p/9123078.html">高并发系统限流-漏桶算法和令牌桶算法</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;高并发三大利器&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存  –  缓存目的是提升系统访问速度和增大系统能处理的容量，可谓是抗高并发流量的银弹&lt;/li&gt;
&lt;li&gt;降级  –  当服务出问题或者影响到核心流程的性能则需要暂时屏蔽掉，待高峰或者问题解决后再</summary>
      
    
    
    
    
    <category term="高并发系统" scheme="http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>高并发系统00之如何设计一个高并发系统</title>
    <link href="http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/"/>
    <id>http://example.com/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/</id>
    <published>2021-07-09T07:08:32.000Z</published>
    <updated>2021-07-09T10:02:59.733Z</updated>
    
    <content type="html"><![CDATA[<p>为啥会有高并发？为啥高并发就很牛逼？</p><p>很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。<br>所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。</p><p>设计一个高并发系统可以简单分为以下 6 点：</p><ul><li>系统拆分</li><li>缓存</li><li>MQ</li><li>分库分表</li><li>读写分离</li><li>ElasticSearch</li></ul><p><img src="/2021/07/09/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F00%E4%B9%8B%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/img_1.png" alt="img.png"></p><h3 id="系统拆分"><a href="#系统拆分" class="headerlink" title="系统拆分"></a>系统拆分</h3><p>将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。</p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。</p><h3 id="MQ"><a href="#MQ" class="headerlink" title="MQ"></a>MQ</h3><p>MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。</p><h3 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h3><p>分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。</p><h3 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h3><p>读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。</p><h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><p>Elasticsearch，简称 ES。</p><p>ES 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为啥会有高并发？为啥高并发就很牛逼？&lt;/p&gt;
&lt;p&gt;很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。&lt;br&gt;所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="高并发系统" scheme="http://example.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>消息中间件Kafka系列01之Kafka为什么这么快</title>
    <link href="http://example.com/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/"/>
    <id>http://example.com/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/</id>
    <published>2021-07-09T06:12:42.000Z</published>
    <updated>2021-07-09T10:02:59.747Z</updated>
    
    <content type="html"><![CDATA[<ul><li>partition 并行处理</li><li>顺序写磁盘，充分利用磁盘特性</li><li>利用了现代操作系统分页存储 Page Cache 来利用内存提高 I/O 效率</li><li>采用了零拷贝技术</li><li>Producer 生产的数据持久化到 broker，采用 mmap 文件映射，实现顺序的快速写入</li><li>Customer 从 broker 读取数据，采用 sendfile，将磁盘文件读到 OS 内核缓冲区后，转到 NIO buffer进行网络发送，减少 CPU 消耗</li></ul><h2 id="详细解读"><a href="#详细解读" class="headerlink" title="详细解读"></a>详细解读</h2><p>无论 kafka 作为 MQ 也好，作为存储层也罢，无非就是两个功能（好简单的样子），一是 Producer 生产的数据存到 broker，二是 Consumer 从 broker 读取数据。那 Kafka 的快也就体现在读写两个方面了，下面我们就聊聊 Kafka 快的原因。</p><h3 id="利用-Partition-实现并行处理"><a href="#利用-Partition-实现并行处理" class="headerlink" title="利用 Partition 实现并行处理"></a>利用 Partition 实现并行处理</h3><p>我们都知道 Kafka 是一个 Pub-Sub 的消息系统，无论是发布还是订阅，都要指定 Topic。</p><p>Topic 只是一个逻辑的概念。每个 Topic 都包含一个或多个 Partition，不同 Partition 可位于不同节点。</p><p>一方面，由于不同 Partition 可位于不同机器，因此可以充分利用集群优势，实现机器间的并行处理。另一方面，由于 Partition 在物理上对应一个文件夹，即使多个 Partition 位于同一个节点，也可通过配置让同一节点上的不同 Partition 置于不同的磁盘上，从而实现磁盘间的并行处理，充分发挥多磁盘的优势。</p><p>能并行处理，速度肯定会有提升，多个工人肯定比一个工人干的快。</p><h3 id="顺序写磁盘"><a href="#顺序写磁盘" class="headerlink" title="顺序写磁盘"></a>顺序写磁盘</h3><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img.png"><br>Kafka 中每个分区是一个有序的，不可变的消息序列，新的消息不断追加到 partition 的末尾，这个就是顺序写。</p><p>由于磁盘有限，不可能保存所有数据，实际上作为消息系统 Kafka 也没必要保存所有数据，需要删除旧的数据。</p><p>又由于顺序写入的原因，所以 Kafka 采用各种删除策略删除数据的时候，并非通过使用“读 - 写”模式去修改文件，而是将 Partition 分为多个 Segment，每个 Segment 对应一个物理文件，通过删除整个文件的方式去删除 Partition 内的数据。这种方式清除旧数据的方式，也避免了对文件的随机写操作。</p><h4 id="简单扯扯磁盘-IO-的那些事"><a href="#简单扯扯磁盘-IO-的那些事" class="headerlink" title="简单扯扯磁盘/IO 的那些事"></a>简单扯扯磁盘/IO 的那些事</h4><p>硬盘性能的制约因素是什么？如何根据磁盘I/O特性来进行系统设计？<br>硬盘内部主要部件为磁盘盘片、传动手臂、读写磁头和主轴马达。<br>实际数据都是写在盘片上，读写主要是通过传动手臂上的读写磁头来完成。实际运行时，主轴让磁盘盘片转动，然后传动手臂可伸展让读取头在盘片上进行读写操作。磁盘物理结构如下图所示：</p><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img2.png"></p><p>由于单一盘片容量有限，一般硬盘都有两张以上的盘片，每个盘片有两面，都可记录信息，所以一张盘片对应着两个磁头。盘片被分为许多扇形的区域，每个区域叫一个扇区。盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用。磁盘盘片垂直视角如下图所示：</p><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img1.png"></p><p>影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。<br>机械硬盘的连续读写性能很好，但随机读写性能很差，这主要是因为磁头移动到正确的磁道上需要时间，随机读写时，磁头需要不停的移动，时间都浪费在了磁头寻址上，所以性能不高。衡量磁盘的重要主要指标是IOPS和吞吐量。<br>在许多的开源框架如 Kafka、HBase 中，都通过追加写的方式来尽可能的将随机 I/O 转换为顺序 I/O，以此来降低寻址时间和旋转延时，从而最大限度的提高 IOPS。  </p><p>感兴趣的同学可以看看 <a href="https://link.zhihu.com/?target=https://tech.meituan.com/2017/05/19/about-desk-io.html">磁盘I/O那些事</a></p><p>磁盘读写的快慢取决于你怎么使用它，也就是顺序读写或者随机读写。</p><h3 id="充分利用-Page-Cache"><a href="#充分利用-Page-Cache" class="headerlink" title="充分利用 Page Cache"></a>充分利用 Page Cache</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在 Linux 的实现中，文件 Cache 分为两个层面，一是 Page Cache，另一个 Buffer Cache。</span><br><span class="line">  每一个 Page Cache 包含若干 Buffer Cache。</span><br><span class="line">* Page Cache 主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有 read/write 操作的时候。</span><br><span class="line">* Buffer Cache 则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。</span><br></pre></td></tr></table></figure><p>使用 Page Cache 的好处：</p><ul><li>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</li><li>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</li><li>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</li><li>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</li><li>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</li></ul><p>Broker 收到数据后，写磁盘时只是将数据写入 Page Cache，并不保证数据一定完全写入磁盘。从这一点看，可能会造成机器宕机时，Page Cache 内的数据未写入磁盘从而造成数据丢失。但是这种丢失只发生在机器断电等造成操作系统不工作的场景，而这种场景完全可以由 Kafka 层面的 Replication 机制去解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。也正因如此，Kafka 虽然提供了 flush.messages 和 flush.ms 两个参数将 Page Cache 中的数据强制 Flush 到磁盘，但是 Kafka 并不建议使用。</p><h3 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h3><p>Kafka 中存在大量的网络数据持久化到磁盘（Producer 到 Broker）和磁盘文件通过网络发送（Broker 到 Consumer）的过程。这一过程的性能直接影响 Kafka 的整体吞吐量。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的权限。</span><br><span class="line">为了避免用户进程直接操作内核，保证内核安全，操作系统将虚拟内存划分为两部分，一部分是内核空间（Kernel-space），一部分是用户空间（User-space）。</span><br></pre></td></tr></table></figure><p>传统的 Linux 系统中，标准的 I/O 接口（例如read，write）都是基于数据拷贝操作的，即 I/O 操作会导致数据在内核地址空间的缓冲区和用户地址空间的缓冲区之间进行拷贝，所以标准 I/O 也被称作缓存 I/O。这样做的好处是，如果所请求的数据已经存放在内核的高速缓冲存储器中，那么就可以减少实际的 I/O 操作，但坏处就是数据拷贝的过程，会导致 CPU 开销。</p><p>我们把 Kafka 的生产和消费简化成如下两个过程来看：</p><ul><li>网络数据持久化到磁盘 (Producer 到 Broker)</li><li>磁盘文件通过网络发送（Broker 到 Consumer）</li></ul><h4 id="1-网络数据持久化到磁盘-Producer-到-Broker"><a href="#1-网络数据持久化到磁盘-Producer-到-Broker" class="headerlink" title="1) 网络数据持久化到磁盘 (Producer 到 Broker)"></a>1) 网络数据持久化到磁盘 (Producer 到 Broker)</h4><p>传统模式下，数据从网络传输到文件需要 4 次数据拷贝、4 次上下文切换和两次系统调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = socket.read()// 读取网络数据 </span><br><span class="line">File file = new File() </span><br><span class="line">file.write(data)// 持久化到磁盘 </span><br><span class="line">file.flush()</span><br></pre></td></tr></table></figure><p>这一过程实际上发生了四次数据拷贝：</p><ul><li>首先通过 DMA copy 将网络数据拷贝到内核态 Socket Buffer</li><li>然后应用程序将内核态 Buffer 数据读入用户态（CPU copy）</li><li>接着用户程序将用户态 Buffer 再拷贝到内核态（CPU copy）</li><li>最后通过 DMA copy 将数据拷贝到磁盘文件</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DMA（Direct Memory Access）：直接存储器访问。DMA 是一种无需 CPU 的参与，让外设和系统内存之间进行双向数据传输的硬件机制。使用 DMA 可以使系统 CPU 从实际的 I/O 数据传输过程中摆脱出来，从而大大提高系统的吞吐率。</span><br></pre></td></tr></table></figure><p>其中伴随着四次上下文切换，如图所示</p><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img3.png"></p><p>数据落盘通常都是非实时的，kafka 生产者数据持久化也是如此。Kafka 的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高 I/O 效率，就是上一节提到的 Page Cache。</p><p>对于 kafka 来说，Producer 生产的数据存到 broker，这个过程读取到 socket buffer 的网络数据，其实可以直接在内核空间完成落盘。并没有必要将 socket buffer 的网络数据，读取到应用进程缓冲区；在这里应用进程缓冲区其实就是 broker，broker 收到生产者的数据，就是为了持久化。</p><p><strong>在此特殊场景下</strong>：接收来自 socket buffer 的网络数据，应用进程不需要中间处理、直接进行持久化时。可以使用 <strong>mmpp</strong> 内存文件映射。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Memory Mapped Files：简称 mmap，也有叫 MMFile 的，使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射。从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程。它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上。</span><br><span class="line">使用这种方式可以获取很大的 I/O 提升，省去了用户空间到内核空间复制的开销。</span><br><span class="line">mmap 也有一个很明显的缺陷——不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘。Kafka 提供了一个参数——producer.type 来控制是不是主动flush；如果 Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步(sync)；写入 mmap 之后立即返回 Producer 不调用 flush 就叫异步(async)，默认是 sync。</span><br></pre></td></tr></table></figure><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img4.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">零拷贝（Zero-copy）技术指在计算机执行操作时，CPU 不需要先将数据从一个内存区域复制到另一个内存区域，从而可以减少上下文切换以及 CPU 的拷贝时间。</span><br><span class="line">它的作用是在数据报从网络设备到用户程序空间传递的过程中，减少数据拷贝次数，减少系统调用，实现 CPU 的零参与，彻底消除 CPU 在这方面的负载。</span><br><span class="line">目前零拷贝技术主要有三种类型：</span><br><span class="line">直接I/O：</span><br><span class="line">        数据直接跨过内核，在用户地址空间与I/O设备之间传递，内核只是进行必要的虚拟存储配置等辅助工作；</span><br><span class="line">避免内核和用户空间之间的数据拷贝：</span><br><span class="line">        当应用程序不需要对数据进行访问时，则可以避免将数据从内核空间拷贝到用户空间</span><br><span class="line">        * mmap</span><br><span class="line">        * sendfile</span><br><span class="line">        * splice &amp;&amp; tee</span><br><span class="line">        * sockmap</span><br><span class="line">copy on write：</span><br><span class="line">        写时拷贝技术，数据不需要提前拷贝，而是当需要修改的时候再进行部分拷贝。</span><br></pre></td></tr></table></figure><h4 id="2-磁盘文件通过网络发送（Broker-到-Consumer）"><a href="#2-磁盘文件通过网络发送（Broker-到-Consumer）" class="headerlink" title="2) 磁盘文件通过网络发送（Broker 到 Consumer）"></a>2) 磁盘文件通过网络发送（Broker 到 Consumer）</h4><p>传统方式实现：先读取磁盘、再用 socket 发送，实际也是进过四次 copy</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">buffer = File.read</span><br><span class="line">Socket.send(buffer)</span><br></pre></td></tr></table></figure><p>这一过程可以类比上边的生产消息：</p><ul><li>首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝）</li><li>然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝）</li><li>接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝）</li><li>最后通过 DMA 拷贝将数据拷贝到 NIC Buffer </li></ul><p>Linux 2.4+ 内核通过 sendfile 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件 - 网络发送由一个 sendfile 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。</p><p><img src="/2021/07/09/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6Kafka%E7%B3%BB%E5%88%9701%E4%B9%8BKafka%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB/img5.png"></p><p>Kafka 在这里采用的方案是通过 NIO 的 transferTo/transferFrom 调用操作系统的 sendfile 实现零拷贝。总共发生 2 次内核数据拷贝、2 次上下文切换和一次系统调用，消除了 CPU 数据拷贝</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>在很多情况下，系统的瓶颈不是 CPU 或磁盘，而是网络IO。</p><p>因此，除了操作系统提供的低级批处理之外，Kafka 的客户端和 broker 还会在通过网络发送数据之前，在一个批处理中累积多条记录 (包括读和写)。记录的批处理分摊了网络往返的开销，使用了更大的数据包从而提高了带宽利用率。</p><h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>Producer 可将数据压缩后发送给 broker，从而减少网络传输代价，目前支持的压缩算法有：Snappy、Gzip、LZ4。数据压缩一般都是和批处理配套使用来作为优化手段的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;partition 并行处理&lt;/li&gt;
&lt;li&gt;顺序写磁盘，充分利用磁盘特性&lt;/li&gt;
&lt;li&gt;利用了现代操作系统分页存储 Page Cache 来利用内存提高 I/O 效率&lt;/li&gt;
&lt;li&gt;采用了零拷贝技术&lt;/li&gt;
&lt;li&gt;Producer 生产的数据持久</summary>
      
    
    
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>数据库MySQL系列01之死锁原理及其解决方案研究</title>
    <link href="http://example.com/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/"/>
    <id>http://example.com/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/</id>
    <published>2021-07-09T05:36:08.000Z</published>
    <updated>2021-07-09T10:02:59.740Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是死锁"><a href="#什么是死锁" class="headerlink" title="什么是死锁"></a>什么是死锁</h1><p>死锁是并发系统中常见的问题，同样也会出现在数据库MySQL的并发读写请求场景中。<br>当两个及以上的事务，双方都在等待对方释放已经持有的锁或因为加锁顺序不一致造成循环等待锁资源，就会出现“死锁”。<br>常见的报错信息为 Deadlock found when trying to get lock…<br>举例来说 A 事务持有 X1 锁 ，申请 X2 锁，B事务持有 X2 锁，申请 X1 锁。A 和 B 事务持有锁并且申请对方持有的锁进入循环等待，就造成了死锁。</p><p><img src="/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/img.png" alt="死锁示例图"></p><p>如上图，是右侧的四辆汽车资源请求产生了回路现象，即死循环，导致了死锁。</p><h1 id="死锁出现要素"><a href="#死锁出现要素" class="headerlink" title="死锁出现要素"></a>死锁出现要素</h1><ul><li>两个或者两个以上事务</li><li>每个事务都已经持有锁并且申请新的锁</li><li>锁资源同时只能被同一个事务持有或者不兼容</li><li>事务之间因为持有锁和申请锁导致彼此循环等待</li></ul><h1 id="经典案例"><a href="#经典案例" class="headerlink" title="经典案例"></a>经典案例</h1><h2 id="案例一-事务并发-insert-唯一键冲突"><a href="#案例一-事务并发-insert-唯一键冲突" class="headerlink" title="案例一:事务并发 insert 唯一键冲突"></a>案例一:事务并发 insert 唯一键冲突</h2><p>表结构如下所示:</p><p><img src="/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/img_1.png" alt="事务并发insert表结构"></p><p>测试用例如下:</p><p><img src="/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/img_2.png" alt="事务并发insert测试用例"></p><p>日志分析如下:</p><ul><li><p>事务 T2 insert into t7(id,a) values (26,10) 语句 insert 成功，持有 a=10 的 排他行锁( Xlocks rec but no gap )</p></li><li><p>事务 T1 insert into t7(id,a) values (30,10), 因为T2的第一条 insert 已经插入 a=10 的记录,事务 T1 insert a=10 则发生唯一键冲突,需要申请对冲突的唯一索引加上S Next-key Lock( 即 lock mode S waiting ) 这是一个间隙锁会申请锁住(,10],(10,20]之间的 gap 区域。</p></li><li><p>事务 T2 insert into t7(id,a) values (40，9)该语句插入的 a=9 的值在事务 T1 申请的 gap 锁4-10之间， 故需事务 T2 的第二条 insert 语句要等待事务 T1 的 S-Next-key Lock 锁释放,在日志中显示 lock_mode X locks gap before rec insert intention waiting 。</p></li></ul><h2 id="案例二-先-update-再-insert-的并发死锁问题"><a href="#案例二-先-update-再-insert-的并发死锁问题" class="headerlink" title="案例二:先 update 再 insert 的并发死锁问题"></a>案例二:先 update 再 insert 的并发死锁问题</h2><p>表结构如下，无数据:</p><p><img src="/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/img_3.png" alt="先update再insert表结构"></p><p>测试用例如下:</p><p><img src="/2021/07/09/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E7%B3%BB%E5%88%9701%E4%B9%8B%E6%AD%BB%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%A0%94%E7%A9%B6/img_4.png" alt="先update再insert测试用例"></p><p>死锁分析:<br>可以看到两个事务 update 不存在的记录，先后获得间隙锁( gap 锁)，gap 锁之间是兼容的所以在update环节不会阻塞。</p><p>两者都持有 gap 锁，然后去竞争插入意向锁。当存在其他会话持有 gap 锁的时候，当前会话申请不了插入意向锁，导致死锁。</p><h1 id="如何尽可能避免死锁"><a href="#如何尽可能避免死锁" class="headerlink" title="如何尽可能避免死锁"></a>如何尽可能避免死锁</h1><ul><li><p>合理的设计索引，区分度高的列放到组合索引前面，使业务 SQL 尽可能通过索引定位更少的行，减少锁竞争。</p></li><li><p>调整业务逻辑 SQL 执行顺序， 避免 update/delete 长时间持有锁的 SQL 在事务前面。</p></li><li><p>避免大事务，尽量将大事务拆成多个小事务来处理，小事务发生锁冲突的几率也更小。</p></li><li><p>以固定的顺序访问表和行。比如两个更新数据的事务，事务 A 更新数据的顺序为 1，2;事务 B 更新数据的顺序为 2，1。这样更可能会造成死锁。</p></li><li><p>在并发比较高的系统中，不要显式加锁，特别是是在事务里显式加锁。如 select … for update 语句，如果是在事务里（运行了 start transaction 或设置了autocommit 等于0）,那么就会锁定所查找到的记录。</p></li><li><p>尽量按主键/索引去查找记录，范围查找增加了锁冲突的可能性，也不要利用数据库做一些额外额度计算工作。比如有的程序会用到 “select … where … order by rand();”这样的语句，由于类似这样的语句用不到索引，因此将导致整个表的数据都被锁住。</p></li><li><p>优化 SQL 和表设计，减少同时占用太多资源的情况。比如说，减少连接的表，将复杂 SQL 分解为多个简单的 SQL。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是死锁&quot;&gt;&lt;a href=&quot;#什么是死锁&quot; class=&quot;headerlink&quot; title=&quot;什么是死锁&quot;&gt;&lt;/a&gt;什么是死锁&lt;/h1&gt;&lt;p&gt;死锁是并发系统中常见的问题，同样也会出现在数据库MySQL的并发读写请求场景中。&lt;br&gt;当两个及以上的事务，双方都在</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="DB" scheme="http://example.com/tags/DB/"/>
    
  </entry>
  
</feed>
